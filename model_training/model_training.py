# -*- coding: utf-8 -*-
"""CreditCardFraudDetection_ModelCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hgpMXSsxkgHI_HK6SINIkVQYJ6j58JZk

### 1. Project Setup and Reproducible Environment

In this section, we initialize the overall project environment and define a consistent folder structure to support an end-to-end ML pipeline.

We start by importing core libraries used throughout the project:
- **Pathlib, os, datetime** for file and path management.
- **NumPy and pandas** for numerical computing and tabular data handling.
- **Matplotlib and Seaborn** for visualization.
- **scikit-learn** utilities for preprocessing, model evaluation, and handling class imbalance.
- **TensorFlow / Keras** for building, training, and evaluating neural network models.
- **opendatasets** to conveniently download the dataset from online sources (e.g., Kaggle) in a reproducible way.
- **joblib** for efficient serialization of models and preprocessing artifacts.

To support **reproducibility**, we define a global random seed and apply it to both NumPy and TensorFlow so that experiments can be re-run with consistent results.

I then define a standardized **project directory structure** under `creditcard-fraud-mlops`, including:
- `data/raw` for immutable raw data downloads,
- `data/processed` for cleaned and transformed datasets,
- `models` for saved model checkpoints and final artifacts,
- `notebooks` for exploratory analysis and experimentation

The helper functions:
- `set_global_seed()` configures all relevant random seeds.
- `create_project_structure()` ensures that all required directories exist before any further processing or training.

Finally, I switch the working directory to the project root so that all subsequent relative paths (for data, and models) remain consistent across environments (e.g., local machine vs. Colab). This setup cell establishes a clean, organized, and reproducible foundation for the rest of the end-to-end ML pipeline.
"""

# ==============================
# 1. Project Setup & Imports
# ==============================

from pathlib import Path
import os
import datetime
import joblib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

try:
    import opendatasets as od
except ImportError:
    import sys
    !{sys.executable} -m pip install opendatasets
    import opendatasets as od

import opendatasets as od
from typing import Tuple, List, Optional, Dict
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import class_weight
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    roc_auc_score,
    precision_recall_curve,
    classification_report,
    confusion_matrix,
    RocCurveDisplay,
    PrecisionRecallDisplay,
)

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, losses, metrics, callbacks


# -----------------------
# Global configuration
# -----------------------

SEED: int = 42
PROJECT_ROOT = Path("creditcard-fraud-mlops")
DATA_DIR = PROJECT_ROOT / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
MODELS_DIR = PROJECT_ROOT / "models"
NOTEBOOKS_DIR = PROJECT_ROOT / "notebooks"
LOGS_DIR = PROJECT_ROOT / "logs"


def set_global_seed(seed: int = 42) -> None:
    """Set random seeds for reproducibility."""
    np.random.seed(seed)
    tf.random.set_seed(seed)


def create_project_structure(root: Path = PROJECT_ROOT) -> None:
    """
    Create standardized project folder structure.

    Parameters
    ----------
    root : Path
        Root directory for the project (default: 'creditcard-fraud-mlops').
    """
    subdirs = [
        RAW_DATA_DIR,
        PROCESSED_DATA_DIR,
        MODELS_DIR,
        NOTEBOOKS_DIR,
        LOGS_DIR,
    ]
    for d in subdirs:
        d.mkdir(parents=True, exist_ok=True)


# -----------------------
# Initialize environment
# -----------------------

set_global_seed(SEED)
create_project_structure()

# Change working directory to project root (for Colab / notebooks)
os.chdir(PROJECT_ROOT)

print(f"Working directory: {Path.cwd()}")
print(f"Data directory:    {DATA_DIR.resolve()}")
print(f"Models directory:  {MODELS_DIR.resolve()}")

"""### 2. Dataset Acquisition and Initial Loading

This section handles the controlled acquisition and loading of the **Credit Card Fraud Detection dataset**, which serves as the foundation for the entire machine learning pipeline.

The dataset is sourced from **Kaggle (MLG–ULB Credit Card Fraud Dataset)** and contains anonymized credit card transactions performed by European cardholders. Each transaction is labeled as either legitimate or fraudulent, making this a highly imbalanced binary classification problem and a strong real-world use case for anomaly and fraud detection.

To ensure **reproducibility and automation**, we encapsulate dataset retrieval and loading logic into dedicated functions:

- `download_creditcard_dataset()`  
  This function checks whether the dataset already exists locally before downloading it. By avoiding repeated downloads, the workflow remains efficient and deterministic across multiple runs and environments. The dataset is stored in the `data/raw` directory, preserving the raw data in an immutable form.

- `load_creditcard_dataframe()`  
  This function validates the presence of the dataset file and loads it into a pandas DataFrame. Basic metadata such as dataset shape is printed to provide immediate feedback on successful loading and to catch potential issues early.

After loading, we perform a **lightweight structural inspection** using `df.info()` and `df.head()` to:
- Verify data types and memory usage,
- Confirm the presence of expected features and the target label,
- Establish an early understanding of the dataset’s dimensionality and schema.

This section deliberately avoids any preprocessing or transformation. By keeping dataset ingestion separate from cleaning and feature engineering, the project maintains a clear and auditable separation between **raw data** and **processed data**, which is essential for reliable experimentation, debugging, and future model retraining.

### 3. Dataset Description (Credit Card Fraud Detection)

- **Source**: Kaggle – “Credit Card Fraud Detection” (European cardholder transactions, September 2013).
- **Rows**: 284,807 transactions.
- **Columns**:
  - `Time`: seconds elapsed between each transaction and the first transaction in the dataset.
  - `V1`–`V28`: numeric features resulting from a PCA transformation applied to the original, confidential cardholder features (to protect privacy).
  - `Amount`: transaction amount.
  - `Class`: target label (0 = legitimate transaction, 1 = fraudulent transaction).

The dataset is **highly imbalanced**: only about 0.17% of transactions are fraudulent. This reflects real-world fraud detection settings, where most transactions are normal and fraud is rare but **high-impact**.

This dataset is relevant because:
- It models a **real, high-stakes** financial security problem.
- The cost of missing fraud (false negatives) is high for banks and customers.
- The cost of flagging legitimate transactions (false positives) is also important because it hurts user experience and trust.

In this project, we build an **end-to-end TensorFlow pipeline** to detect fraudulent transactions and handle **class imbalance**, **model evaluation**, and **deployment readiness**.
"""

# ========================================
# 2. Download & Load Credit Card Dataset
# ========================================


def download_creditcard_dataset(url: str, download_dir: Path) -> Path:
    """
    Download the Kaggle credit card fraud dataset if not already present.

    Parameters
    ----------
    url : str
        Kaggle dataset URL.
    download_dir : Path
        Local directory where the dataset will be downloaded.

    Returns
    -------
    Path
        Path to the downloaded CSV file.
    """
    download_dir.mkdir(parents=True, exist_ok=True)

    # Only download if not already present
    csv_path = download_dir / "creditcardfraud" / "creditcard.csv"
    if not csv_path.exists():
        print("Downloading dataset...")
        od.download(url, str(download_dir))
    else:
        print("Dataset already exists. Skipping download.")

    return csv_path


def load_creditcard_dataframe(csv_path: Path) -> pd.DataFrame:
    """
    Load the credit card fraud dataset CSV into a pandas DataFrame.

    Parameters
    ----------
    csv_path : Path
        Path to the dataset CSV.

    Returns
    -------
    DataFrame
        Loaded dataset.
    """
    if not csv_path.exists():
        raise FileNotFoundError(f"CSV not found at: {csv_path}")

    df = pd.read_csv(csv_path)
    print(f"Loaded dataset with shape: {df.shape}")
    return df


# -------- Execute the workflow -------- #

DATA_URL = "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
csv_path = download_creditcard_dataset(DATA_URL, RAW_DATA_DIR)

df = load_creditcard_dataframe(csv_path)

# Quick overview
print("\nDataset Info:")
print(df.info())
df.head()

"""### 4. Stratified Subsampling for Efficient Experimentation

The full Credit Card Fraud Detection dataset contains nearly 285,000 transactions, which can slow down iterative experimentation—especially during model prototyping, hyperparameter tuning, and debugging.

To support **faster experimentation without distorting the learning problem**, we create a **stratified subsample** of the dataset. Stratification ensures that the severe class imbalance inherent to fraud detection is preserved in the reduced dataset, allowing experimental results to remain representative of the original data distribution.

The `stratified_subsample()` function:
- Separates fraudulent and non-fraudulent transactions,
- Computes the number of samples to draw from each class based on the original class ratio,
- Randomly samples from each class using a fixed seed for reproducibility,
- Shuffles the combined subset to avoid ordering artifacts.

Before subsampling, the original class distribution and fraud ratio are printed to establish a baseline. After subsampling, the same statistics are reported again to explicitly verify that the class imbalance is preserved.

The resulting subset of **100,000 transactions** is saved to the `data/processed` directory. This aligns with best practices by keeping transformed datasets separate from raw data and enables:
- Faster model iteration during development,
- Reproducible experiments using a fixed, versionable processed dataset,
- Seamless transition to full-dataset training once the pipeline is validated.

This step reflects a realistic ML workflow in which developers balance computational efficiency with statistical integrity during model development.

"""

# ========================================
# 4. Subsample for Faster Experimentation
# ========================================

def stratified_subsample(
    df: pd.DataFrame,
    target_col: str = "Class",
    subset_size: int = 100_000,
    seed: int = 42,
) -> pd.DataFrame:
    """
    Create a stratified subset of the dataset while preserving the class ratio.

    Parameters
    ----------
    df : DataFrame
        Full credit card fraud dataset.
    target_col : str
        Name of the target label column (default "Class").
    subset_size : int
        Desired number of rows in the subsampled dataset.
    seed : int
        Random seed for reproducibility.

    Returns
    -------
    DataFrame
        Stratified and shuffled subsampled DataFrame.
    """
    # Separate fraud vs non-fraud
    df_pos = df[df[target_col] == 1]
    df_neg = df[df[target_col] == 0]

    # Number of samples proportional to original class ratio
    n_pos = int(subset_size * len(df_pos) / len(df))
    n_neg = subset_size - n_pos

    print(f"Sampling {n_pos} fraud and {n_neg} non-fraud rows...")

    pos_sample = df_pos.sample(n=n_pos, random_state=seed)
    neg_sample = df_neg.sample(n=n_neg, random_state=seed)

    # Combine and shuffle
    subset = (
        pd.concat([pos_sample, neg_sample])
        .sample(frac=1, random_state=seed)
        .reset_index(drop=True)
    )

    return subset


# --------- EXECUTE SUBSAMPLING --------- #

print("Original class distribution:")
print(df["Class"].value_counts())
print(f"Fraud ratio: {df['Class'].mean():.6f}\n")

subset_size = 100_000
df_subset = stratified_subsample(df, subset_size=subset_size, seed=SEED)

print("Subset shape:", df_subset.shape)
print("Subset class distribution:")
print(df_subset["Class"].value_counts())
print("Subset fraud ratio:", df_subset["Class"].mean())

# Ensure directory exists
PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)

# Save to processed directory
subset_path = PROCESSED_DATA_DIR / "creditcard_subset_100k.csv"
df_subset.to_csv(subset_path, index=False)

print(f"\nSaved subsampled dataset to: {subset_path.resolve()}")

"""### 5. Data Cleaning and Feature Engineering

Once the subsampled dataset is stored in the `data/processed` directory, the next step is to perform **basic data cleaning** and apply **simple, interpretable feature engineering** to support downstream modeling.

In this section, we structure the preprocessing as a small, reusable pipeline:

- `load_processed_subset()`  
  Loads the previously saved subsampled CSV from disk and validates that the file exists. This keeps the workflow modular and ensures that future runs (or deployments) can start directly from the processed dataset without repeating raw data ingestion and subsampling.

- `inspect_numeric_columns()`  
  Provides a quick diagnostic of the dataset’s numeric columns and total number of missing values. This is a lightweight sanity check that confirms the overall schema and helps catch unexpected data quality issues before modeling.

- `remove_duplicates()`  
  Identifies and removes any duplicate rows in the dataset, printing both the number of duplicates found and the new shape after de-duplication. Although this dataset is generally clean, this step reflects good practice for real-world pipelines where duplicate records can bias model training.

- `feature_engineering()`  
  Applies a set of **meaningful yet simple transformations**:
  - `log_amount`: a `log1p`-transformed version of `Amount` to reduce the impact of extreme transaction values and stabilize the distribution.
  - `hour`: an approximate hour-of-day feature derived from the `Time` variable (in seconds), capturing potential temporal patterns in fraud (e.g., time-of-day risk).
  - `is_night`: a binary indicator for night-time transactions (00:00–05:59), which can capture behavioral differences between daytime and nighttime activity.

The original DataFrame is copied before feature engineering to avoid unintended side effects, and key columns (`Amount`, `Time`) are validated to be present before transformations are applied. Finally, the newly engineered features are previewed alongside the original columns to verify that the transformations behaved as expected.

This step contributes to the **model training pipeline** by:
- Encapsulating preprocessing logic in clearly defined functions,
- Making data cleaning and feature creation reproducible and easy to extend,
- Preparing both scale-adjusted and behaviorally motivated features for the TensorFlow model.
"""

# ========================================
# 5. Data Cleaning & Feature Engineering
# ========================================


def load_processed_subset(path: Path) -> pd.DataFrame:
    """
    Load the subsampled dataset from disk.

    Parameters
    ----------
    path : Path
        Path to the CSV file.

    Returns
    -------
    DataFrame
        Loaded subset.
    """
    if not path.exists():
        raise FileNotFoundError(f"Processed subset not found at: {path}")

    df = pd.read_csv(path)
    print(f"Loaded subset with shape: {df.shape}")
    return df


def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """
    Remove duplicate rows from the dataset if they exist.

    Parameters
    ----------
    df : DataFrame
        Input dataset.

    Returns
    -------
    DataFrame
        Dataset with duplicates removed.
    """
    dup_count = df.duplicated().sum()
    print(f"Duplicate rows detected: {dup_count}")

    if dup_count > 0:
        df = df.drop_duplicates().reset_index(drop=True)
        print(f"Removed duplicates. New shape: {df.shape}")

    return df


def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:
    """
    Apply simple but meaningful feature engineering:
      - log_amount: log1p-transformed Amount
      - hour: approximate hour extracted from Time
      - is_night: binary flag for night-time transactions

    Parameters
    ----------
    df : DataFrame
        Input dataset.

    Returns
    -------
    DataFrame
        Dataset with new features added.
    """
    df = df.copy()

    # Safe numeric check
    if "Amount" not in df.columns or "Time" not in df.columns:
        raise KeyError("Expected columns 'Amount' and 'Time' not found in dataframe.")

    df["log_amount"] = np.log1p(df["Amount"])
    df["hour"] = (df["Time"] // 3600) % 24
    df["is_night"] = df["hour"].isin(range(0, 6)).astype(int)

    return df


def inspect_numeric_columns(df: pd.DataFrame):
    """
    Print useful debugging info about numeric columns.

    Parameters
    ----------
    df : DataFrame
        Input dataset.
    """
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    print("Numeric columns detected:", numeric_cols)
    print("Total missing values:", df.isna().sum().sum())


# -------- EXECUTION PIPELINE -------- #

subset_path = PROCESSED_DATA_DIR / "creditcard_subset_100k.csv"
df = load_processed_subset(subset_path)

# Validate initial structure
inspect_numeric_columns(df)

# Deduplicate
df = remove_duplicates(df)

# Feature engineering
df = feature_engineering(df)

# Preview
df[["Amount", "log_amount", "Time", "hour", "is_night"]].head()

"""### 6. Exploratory Data Analysis: Severity of Class Imbalance

A critical first step in exploratory data analysis for fraud detection is understanding **how imbalanced the target variable is**. Class imbalance directly affects model training, evaluation strategy, and metric selection, and ignoring it can lead to misleading performance results.

In this analysis, we examine the distribution of the target variable (`Class`) by:
- Visualizing the number of legitimate (`Class = 0`) and fraudulent (`Class = 1`) transactions using a count plot.
- Computing the **fraud ratio**, defined as the proportion of fraudulent transactions in the dataset.

This visualization provides an immediate, intuitive understanding of how rare fraudulent transactions are relative to legitimate ones. The fraud ratio quantifies this imbalance numerically and confirms whether the subsampled dataset preserves the extreme skew present in the original data.

Understanding the severity of class imbalance at this stage is essential, as it motivates several downstream design decisions, including:
- The use of **class weighting** or other imbalance-handling techniques during model training,
- The choice of evaluation metrics beyond accuracy (e.g., Precision, Recall, PR-AUC),
- Cautious interpretation of confusion matrices and model predictions.

This step establishes the fundamental challenge of the problem: detecting rare but high-impact fraudulent events within a vast majority of normal transactions.
"""

# Visualize class distribution
sns.countplot(x=df["Class"])
plt.title("Class Distribution")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

# Compute and display fraud ratio
fraud_ratio = df["Class"].mean()
print(f"Fraud Ratio: {fraud_ratio:.4f}")

"""**Observation – Class Imbalance**

The dataset exhibits **extreme class imbalance**. Fraudulent transactions (`Class = 1`) account for only **0.16%** of all observations, while legitimate transactions (`Class = 0`) dominate the dataset.

This confirms that fraud detection is a **rare-event classification problem**, where accuracy alone would be misleading. Handling class imbalance explicitly and using appropriate evaluation metrics (e.g., Precision, Recall, PR-AUC) is therefore essential in later modeling stages.

### 7. Exploratory Data Analysis: Transaction Amounts by Class

After examining class imbalance, the next step is to understand how **transaction amounts differ between fraudulent and legitimate transactions**. Transaction amount is a key behavioral feature and may provide useful signal for distinguishing fraud, especially when combined with other attributes.

In this analysis, we compare the distribution of the `Amount` feature across classes by:
- Visualizing transaction amounts for legitimate (`Class = 0`) and fraudulent (`Class = 1`) transactions using a boxplot,
- Applying a **logarithmic scale** to the y-axis to account for heavy right skew and extreme outlier values.

Using a log scale enables clearer comparison of central tendencies and variability between the two classes, which would otherwise be dominated by a small number of very large transactions.

This analysis helps assess whether transaction size alone is informative and supports subsequent modeling decisions such as:
- Log-transforming the `Amount` feature during preprocessing,
- Combining amount-related features with temporal and behavioral features rather than relying on amount in isolation.

Understanding these distributional differences ensures that feature engineering choices are grounded in empirical patterns observed during exploratory analysis.
"""

# Boxplot: Transaction amount distribution by class (log scale)
sns.boxplot(x="Class", y="Amount", data=df)

plt.yscale("log")
plt.title("Amount vs Class (Log Scale)")
plt.xlabel("Class")
plt.ylabel("Amount (Log Scale)")
plt.show()

"""**Observation – Transaction Amounts by Class**

The boxplot shows that transaction amounts are **right-skewed** for both fraudulent and non-fraudulent transactions, justifying the use of a log scale. Fraudulent transactions (`Class = 1`) tend to have a **lower median amount** but a **wider spread**, indicating that fraud occurs across a broad range of transaction values.

While legitimate transactions (`Class = 0`) are more concentrated at moderate amounts, both classes exhibit significant overlap. This suggests that transaction amount alone is insufficient for discrimination, but it provides useful signal when combined with other temporal and behavioral features in the model.

### 8. Exploratory Data Analysis: Temporal Patterns in Fraud Transactions

Beyond transaction amount, temporal behavior can provide important context for fraud detection. Certain types of fraudulent activity may be more likely to occur at specific times, reflecting opportunistic behavior or reduced oversight during particular periods.

In this analysis, we examine whether fraudulent transactions occur at distinct times compared to legitimate transactions by:
- Plotting kernel density estimates (KDEs) of the `Time` feature for both fraudulent and non-fraudulent transactions,
- Comparing the overall temporal distribution shapes rather than individual transaction timestamps.

The KDE plots allow smooth comparison of density across time and help identify shifts, peaks, or concentration differences between the two classes. Understanding temporal patterns at this stage helps assess whether time alone carries discriminative information for fraud detection.
"""

# KDE plot for transaction time distribution
plt.figure(figsize=(10, 5))

sns.kdeplot(
    df[df["Class"] == 0]["Time"],
    label="Non-Fraud",
    bw_adjust=1
)
sns.kdeplot(
    df[df["Class"] == 1]["Time"],
    label="Fraud",
    bw_adjust=1
)

plt.title("Transaction Time Distribution")
plt.xlabel("Time")
plt.ylabel("Density")
plt.legend()
plt.tight_layout()
plt.show()

"""**Observation – Temporal Patterns in Fraud**

The KDE plot shows that fraudulent transactions (orange) occur throughout the entire time range but follow a **smoother and more evenly distributed pattern** compared to non-fraud transactions (blue), which exhibit sharper peaks and more pronounced periodic activity.

This suggests that fraud does not concentrate at a single specific time, but its timing differs subtly from normal transaction behavior. While time alone is insufficient for clear separation, these differences indicate that temporal features (e.g., hour of day or night-time indicators) can provide useful complementary signal when combined with other features.

### 9. Exploratory Data Analysis: Correlation Structure of Features

To better understand relationships between features in the dataset, we compute and visualize the **correlation matrix** for all numerical variables using a heatmap.

This plot helps to:
- Identify groups of features that are **strongly correlated** (e.g., some of the PCA components `V1–V28`),
- Detect potential **redundancy** or multicollinearity among features,
- Reveal how the target label (`Class`) correlates with individual features at a linear level.

Although the PCA-transformed components are already decorrelated to some extent, visualizing the correlation structure provides a useful sanity check and helps interpret how engineered features (such as `log_amount`, `hour`, and `is_night`) relate to the rest of the feature space. This overview informs later modeling choices and regularization strategies.
"""

# Correlation heatmap of all numerical features
plt.figure(figsize=(12, 8))

sns.heatmap(
    df.corr(),
    cmap="coolwarm",
    annot=False,
    linewidths=0.5
)

plt.title("Correlation Heatmap")
plt.xlabel("Features")
plt.ylabel("Features")
plt.tight_layout()
plt.show()

"""**Observation – Feature Correlations**

The correlation heatmap shows that most features have **low pairwise correlations**, particularly among the PCA-transformed variables (`V1–V28`), indicating limited multicollinearity. This is expected, as these features were constructed to be largely decorrelated.

The target variable (`Class`) exhibits **weak linear correlation** with any single feature, highlighting that fraud detection cannot rely on one feature alone. Engineered features such as `log_amount`, `hour`, and `is_night` show localized correlations but remain largely independent.

Overall, this suggests that fraud detection in this dataset relies on **complex interactions between multiple features**, reinforcing the need for non-linear models such as neural networks rather than simple linear classifiers.

### 10. Exploratory Data Analysis: Fraud Rate Across Transaction Amounts

After examining overall amount distributions, the next question is whether **higher-value transactions are more likely to be fraudulent**. Rather than comparing raw amounts directly, we analyze fraud likelihood across transaction amount ranges.

In this analysis, we:
- Bin the `Amount` feature into **10 equal-frequency (quantile-based) bins**, ensuring that each bin contains a comparable number of transactions,
- Compute the **fraud rate** within each bin,
- Visualize fraud rate by amount decile using a bar plot.

Using equal-frequency bins avoids bias from the heavy skew in transaction amounts and allows fair comparison across low- and high-value transactions. This analysis helps determine whether transaction magnitude meaningfully influences fraud risk and whether amount-related features should play a stronger role in modeling.
"""

# Bin the 'Amount' feature into 10 equal-frequency bins
df["Amount_bin"] = pd.qcut(df["Amount"], q=10)

# Convert bins to string for cleaner plotting
df["Amount_bin_str"] = df["Amount_bin"].astype(str)

# Plot fraud rate by amount decile
plt.figure(figsize=(12, 6))
sns.barplot(
    x="Amount_bin_str",
    y="Class",
    data=df,
    estimator=lambda x: x.mean()
)

plt.xticks(rotation=90)
plt.xlabel("Amount Decile (Equal Frequency Bins)")
plt.ylabel("Fraud Rate")
plt.title("Fraud Rate by Amount Decile")
plt.tight_layout()
plt.show()

"""**Observation – High-Value Transactions and Fraud**

The fraud rate varies across amount deciles, showing that **higher transaction value does not consistently correspond to higher fraud likelihood**. While some mid-to-high amount bins exhibit elevated fraud rates, the relationship is non-linear and non-monotonic.

This indicates that fraud is not driven solely by transaction size. High-value transactions are not inherently fraudulent, and low-value transactions can also carry risk. Transaction amount provides useful contextual signal, but effective fraud detection requires combining amount-related information with temporal and behavioral features.

### 11. Exploratory Data Analysis: Transaction Speed Differences

In addition to transaction amount and timing, **transaction speed** can provide insight into fraudulent behavior. Rapid sequences of transactions may indicate automated or suspicious activity, whereas legitimate transactions often occur with more natural temporal spacing.

In this analysis, we examine whether fraud and non-fraud transactions differ in transaction speed by:
- Sorting transactions chronologically using the `Time` feature,
- Computing the time difference between consecutive transactions,
- Comparing the distribution of these time gaps for fraudulent and legitimate transactions using a boxplot.

A **logarithmic scale** is applied to the time differences due to the highly skewed nature of inter-transaction gaps, which range from very short intervals to long pauses. This analysis helps assess whether fraud is associated with unusually fast or irregular transaction patterns and motivates the inclusion of timing-based features in later modeling stages.
"""

# Sort transactions by time and compute time difference
df_sorted = df.sort_values("Time").copy()
df_sorted["Time_Diff"] = df_sorted["Time"].diff()

# Boxplot of time gaps between transactions (log scale)
plt.figure(figsize=(10, 5))

sns.boxplot(
    x=df_sorted["Class"],
    y=df_sorted["Time_Diff"]
)

plt.yscale("log")
plt.title("Time Difference Between Transactions")
plt.xlabel("Class")
plt.ylabel("Time Difference (Log Scale)")
plt.tight_layout()
plt.show()

"""**Observation – Transaction Speed Differences**

The boxplot of inter-transaction time gaps (log scale) shows that fraudulent transactions (`Class = 1`) tend to have **larger and more variable time differences** compared to non-fraud transactions. Legitimate transactions (`Class = 0`) are more tightly clustered around shorter time gaps.

This suggests that fraud does not consistently occur in rapid bursts; instead, fraudulent activity often exhibits **irregular or delayed transaction timing**. Transaction speed alone is not a definitive indicator of fraud, but it provides useful temporal context when combined with other behavioral and amount-based features.

### 12. Post-EDA Cleanup and Dataset Finalization

After completing exploratory data analysis, the dataset contains temporary columns created solely for visualization and analysis purposes (e.g., binned amount intervals). These features are useful for interpretation but are **not suitable for model training** and should not be carried forward into the pipeline.

In this step, we:
- Identify columns with an *interval data type* created during EDA,
- Explicitly remove auxiliary EDA-only columns such as `Amount_bin` and `Amount_bin_str`,
- Safely drop these columns while ignoring any that may not be present.

This cleanup step ensures that the final dataset used for model training contains only **valid numerical and engineered features**, avoids data leakage from exploratory constructs, and maintains a clean separation between EDA artifacts and modeling inputs.
"""

# Remove interval-type columns created during EDA
interval_cols = [
    col for col in df.columns
    if pd.api.types.is_interval_dtype(df[col])
]

cols_to_drop = interval_cols.copy()

# Remove Amount_bin if present
if "Amount_bin" in df.columns:
    cols_to_drop.append("Amount_bin")

# Remove Amount_bin_str if present
if "Amount_bin_str" in df.columns:
    cols_to_drop.append("Amount_bin_str")

# Drop all collected columns safely
df = df.drop(columns=cols_to_drop, errors="ignore")

print("Removed columns:", cols_to_drop)

"""### 13. Final Dataset Sanity Check

Before proceeding to train–validation splitting and model training, we perform a final inspection of the cleaned dataset using `df.head()`.

This step serves as a **sanity check** to confirm that:
- All EDA-only columns have been successfully removed,
- Engineered features (e.g., `log_amount`, `hour`, `is_night`) are present,
- The dataset contains only valid, model-ready features with the expected structure.

Verifying the dataset at this stage helps prevent silent errors from propagating into the modeling pipeline and ensures that training begins from a clean, well-defined input.
"""

df.head()

"""### 14. Train/Validation/Test Split and Feature Scaling

With the cleaned and engineered dataset ready, the next step is to prepare **separate data splits** for model development and evaluation, and to **normalize numeric features** for stable neural network training.

We structure this step into two reusable functions:

- `split_dataset()`  
  Performs a **stratified** split of the data into:
  - 70% training,
  - 15% validation,
  - 15% test.

  Stratification on the `Class` label preserves the original fraud ratio in each split, which is crucial for an imbalanced classification problem. The training set is used for learning model parameters, the validation set for hyperparameter tuning and early stopping, and the test set is held out for final, unbiased performance evaluation.

- `scale_numeric_features()`  
  Detects all numeric columns and applies **StandardScaler**:
  - The scaler is **fit only on the training data** to avoid data leakage.
  - The learned scaling parameters are then applied to the validation and test sets.
  - The function returns the scaled splits, the list of numeric feature names, and the fitted scaler object.

Feature scaling ensures that all input features are on a comparable scale, which helps gradient-based optimization in TensorFlow converge more reliably and prevents features with larger numeric ranges from dominating the learning process.

Finally, we define `FEATURE_COLS` based on the scaled training DataFrame columns. This list will be used to configure the TensorFlow model’s input dimension and to keep the preprocessing and modeling steps tightly aligned.
"""

# ========================================
# 6. Train/Val/Test Split + Scaling
# ========================================


def split_dataset(
    df: pd.DataFrame,
    target_col: str = "Class",
    seed: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame,
           pd.Series, pd.Series, pd.Series]:
    """
    Perform a stratified train/validation/test split:
        - 70% train
        - 15% validation
        - 15% test

    Parameters
    ----------
    df : DataFrame
        Full processed dataset with engineered features.
    target_col : str
        The name of the target column.
    seed : int
        Random seed for reproducibility.

    Returns
    -------
    X_train, X_val, X_test, y_train, y_val, y_test
    """
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # First split: 70% train, 30% temp
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y,
        test_size=0.30,
        stratify=y,
        random_state=seed
    )

    # Second split: 15% val, 15% test
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp,
        test_size=0.50,
        stratify=y_temp,
        random_state=seed
    )

    print(f"Train shape: {X_train.shape}")
    print(f"Val shape:   {X_val.shape}")
    print(f"Test shape:  {X_test.shape}")

    return X_train, X_val, X_test, y_train, y_val, y_test



def scale_numeric_features(
    X_train: pd.DataFrame,
    X_val: pd.DataFrame,
    X_test: pd.DataFrame
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, List[str], StandardScaler]:
    """
    Scale numeric features using StandardScaler (fit only on train).

    Parameters
    ----------
    X_train, X_val, X_test : DataFrames
        Input split datasets.

    Returns
    -------
    X_train_scaled, X_val_scaled, X_test_scaled, numeric_cols, scaler
    """
    # Detect numeric columns
    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
    print("\nNumeric columns to be scaled:", numeric_cols)

    scaler = StandardScaler()

    # Copy to avoid mutation issues
    X_train_scaled = X_train.copy()
    X_val_scaled = X_val.copy()
    X_test_scaled = X_test.copy()

    # Fit only on training data
    X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
    X_val_scaled[numeric_cols] = scaler.transform(X_val[numeric_cols])
    X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])

    print("Scaling complete.")

    return (
        X_train_scaled,
        X_val_scaled,
        X_test_scaled,
        numeric_cols,
        scaler
    )


# -------- EXECUTION PIPELINE -------- #

X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(df, seed=SEED)

(
    X_train_scaled,
    X_val_scaled,
    X_test_scaled,
    numeric_cols,
    scaler
) = scale_numeric_features(X_train, X_val, X_test)

# Define feature columns for model input
FEATURE_COLS = X_train_scaled.columns.tolist()
print("FEATURE_COLS defined:", FEATURE_COLS)
print("Input dim:", len(FEATURE_COLS))

# Preview
X_train_scaled.head()

"""### 15. Persisting Processed Data Splits and Preprocessing Artifacts

To support reproducibility, modular experimentation, and future deployment, the processed datasets and preprocessing objects must be **persisted to disk**. This ensures that model training, evaluation, and inference can all rely on the **exact same data transformations**.

In this section, we save:

- **Train, validation, and test splits** as separate CSV files:
  - Each split combines scaled feature values with the target label (`Class`),
  - Files are stored in the `data/processed` directory to clearly distinguish them from raw data.

- **The fitted `StandardScaler` object**:
  - Saved using `joblib` to the `models` directory,
  - Enables consistent feature scaling during future inference and model deployment,
  - Prevents retraining-time and inference-time preprocessing mismatch.

By explicitly persisting both data splits and preprocessing artifacts, this step strengthens the end-to-end pipeline by ensuring **reproducibility, auditability, and deployment readiness**.
"""

# ========================================
# 7. Save Processed Splits & Scaler
# ========================================

def save_split_datasets(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_val: pd.DataFrame,
    y_val: pd.Series,
    X_test: pd.DataFrame,
    y_test: pd.Series,
    processed_dir: Path = PROCESSED_DATA_DIR
) -> None:
    """
    Save train/validation/test data splits as CSV files.

    Parameters
    ----------
    X_train, X_val, X_test : DataFrame
        Feature matrices for each split.
    y_train, y_val, y_test : Series
        Target labels.
    processed_dir : Path
        Directory where processed CSVs will be stored.
    """
    processed_dir.mkdir(parents=True, exist_ok=True)

    # Combine features + labels
    train_df = X_train.copy()
    train_df["Class"] = y_train.values

    val_df = X_val.copy()
    val_df["Class"] = y_val.values

    test_df = X_test.copy()
    test_df["Class"] = y_test.values

    # File paths
    train_path = processed_dir / "train.csv"
    val_path = processed_dir / "val.csv"
    test_path = processed_dir / "test.csv"

    # Save to disk
    train_df.to_csv(train_path, index=False)
    val_df.to_csv(val_path, index=False)
    test_df.to_csv(test_path, index=False)

    print(f"Saved: {train_path}")
    print(f"Saved: {val_path}")
    print(f"Saved: {test_path}")


def save_scaler(scaler, model_dir: Path = MODELS_DIR) -> None:
    """
    Save the fitted StandardScaler to disk using joblib.

    Parameters
    ----------
    scaler : StandardScaler
        The fitted scaler.
    model_dir : Path
        Directory where scaler.pkl will be saved.
    """
    model_dir.mkdir(parents=True, exist_ok=True)
    scaler_path = model_dir / "scaler.pkl"
    joblib.dump(scaler, scaler_path)
    print(f"Saved scaler to: {scaler_path}")


# -------- EXECUTE SAVING -------- #

save_split_datasets(
    X_train_scaled, y_train,
    X_val_scaled, y_val,
    X_test_scaled, y_test,
)

save_scaler(scaler)

print("\nAll processed splits and scaler have been saved successfully.")

"""### 16. TensorFlow Input Pipelines with `tf.data`

To train the neural network efficiently and reproducibly, we wrap the preprocessed NumPy/pandas data into **TensorFlow `tf.data` input pipelines**. This provides a scalable, composable way to feed data into the model during training, validation, and testing.

We define two small utilities:

- `build_tf_dataset()`  
  Converts a pandas `DataFrame` of features and a `Series` of labels into a `tf.data.Dataset` and applies:
  - Type conversion to TensorFlow-friendly dtypes (`float32` for features, `int32` for labels),
  - Optional shuffling (with a fixed seed for reproducibility),
  - Optional caching to speed up repeated epochs,
  - Batching and prefetching (`AUTOTUNE`) to overlap data loading with model execution.

  This function is used to construct separate datasets for training, validation, and testing, each with appropriate shuffling and caching configurations.

- `describe_dataset()`  
  Prints a brief summary (number of samples and feature dimension) for each split, serving as a sanity check before training.

Using `tf.data` pipelines instead of feeding raw NumPy arrays improves **throughput**, **scalability**, and makes the overall training setup more aligned with production-ready TensorFlow workflows.
"""

# ========================================
# 8. TensorFlow Input Pipelines (tf.data)
# ========================================

tf.random.set_seed(SEED)


def build_tf_dataset(
    X: pd.DataFrame,
    y: pd.Series,
    batch_size: int = 2048,
    shuffle: bool = False,
    cache: bool = False
) -> tf.data.Dataset:
    """
    Create a TensorFlow Dataset pipeline for efficient model training.

    Parameters
    ----------
    X : DataFrame
        Feature matrix.
    y : Series
        Target labels.
    batch_size : int
        Batch size for training.
    shuffle : bool
        Whether to shuffle the dataset.
    cache : bool
        Whether to cache the dataset in memory.

    Returns
    -------
    tf.data.Dataset
        Prepared dataset for training/evaluation.
    """
    # Convert to TF-friendly numpy types
    features = X.values.astype("float32")
    labels = y.values.astype("int32")

    ds = tf.data.Dataset.from_tensor_slices((features, labels))

    if shuffle:
        ds = ds.shuffle(
            buffer_size=len(X),
            seed=SEED,
            reshuffle_each_iteration=True
        )

    if cache:
        ds = ds.cache()

    ds = (
        ds.batch(batch_size)
          .prefetch(tf.data.AUTOTUNE)
    )

    return ds


def describe_dataset(name: str, X: pd.DataFrame, y: pd.Series):
    """Utility to print dataset metadata."""
    print(f"{name} → Samples: {len(X)}, Features: {X.shape[1]}")


# -----------------------------------------
# Build datasets
# -----------------------------------------

describe_dataset("Train", X_train_scaled, y_train)
describe_dataset("Validation", X_val_scaled, y_val)
describe_dataset("Test", X_test_scaled, y_test)

train_ds = build_tf_dataset(
    X_train_scaled,
    y_train,
    batch_size=2048,
    shuffle=True,
    cache=True
)

val_ds = build_tf_dataset(
    X_val_scaled,
    y_val,
    batch_size=2048,
    shuffle=False,
    cache=True
)

test_ds = build_tf_dataset(
    X_test_scaled,
    y_test,
    batch_size=2048,
    shuffle=False,
    cache=False
)

train_ds

"""### 17. Baseline TensorFlow Model (Logistic Regression Equivalent)

Before building more complex neural networks, we first train a **simple baseline model** to establish a reference point for performance. This helps answer whether added depth and complexity meaningfully improve results beyond a linear decision boundary.

We define two functions:

- `build_baseline_model(input_dim)`  
  Builds a **single-layer neural network** equivalent to logistic regression:
  - Input layer with `input_dim` features (after preprocessing and scaling),
  - Single-unit dense output layer with a `sigmoid` activation to produce fraud probabilities,
  - Compiled with:
    - **Adam optimizer** (`learning_rate = 1e-3`),
    - **Binary cross-entropy** loss,
    - Evaluation metrics: AUC, Precision, and Recall — all appropriate for an imbalanced binary classification task.

- `train_baseline_model(...)`  
  Wraps `model.fit()` into a reusable training function that:
  - Trains on the `train_ds` `tf.data` pipeline,
  - Evaluates on the `val_ds` validation dataset,
  - Supports configurable number of epochs and optional Keras callbacks (e.g., early stopping).

This baseline model provides a **simple, interpretable benchmark** against which we will later compare deeper architectures that can capture more complex, non-linear relationships in the data. If a more complex model does not substantially outperform this baseline, its added complexity may not be justified.
"""

# ========================================
# 9. Baseline TensorFlow Model
# ========================================


def build_baseline_model(input_dim: int) -> tf.keras.Model:
    """
    Build a baseline TensorFlow model (similar to logistic regression).

    Parameters
    ----------
    input_dim : int
        Number of input features after preprocessing.

    Returns
    -------
    tf.keras.Model
        Compiled baseline model.
    """
    inputs = layers.Input(shape=(input_dim,), name="input_layer")

    # Linear model → logistic regression in TF
    outputs = layers.Dense(
        units=1,
        activation="sigmoid",
        name="output_layer"
    )(inputs)

    model = models.Model(inputs=inputs, outputs=outputs, name="baseline_model")

    model.compile(
        optimizer=optimizers.Adam(learning_rate=1e-3),
        loss=losses.BinaryCrossentropy(),
        metrics=[
            metrics.AUC(name="auc"),
            metrics.Precision(name="precision"),
            metrics.Recall(name="recall"),
        ]
    )

    return model


def train_baseline_model(
    model: tf.keras.Model,
    train_ds: tf.data.Dataset,
    val_ds: tf.data.Dataset,
    epochs: int = 5,
    callbacks_list: Optional[list] = None
) -> tf.keras.callbacks.History:
    """
    Train the baseline model.

    Parameters
    ----------
    model : tf.keras.Model
        Baseline model instance.
    train_ds : tf.data.Dataset
        Training dataset.
    val_ds : tf.data.Dataset
        Validation dataset.
    epochs : int
        Number of epochs for training.
    callbacks_list : list, optional
        Keras callbacks.

    Returns
    -------
    History
        Training history object.
    """
    print("\nTraining Baseline Model...\n")

    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        verbose=2,
        callbacks=callbacks_list
    )

    return history


# -------- EXECUTION PIPELINE -------- #

baseline_model = build_baseline_model(input_dim=len(FEATURE_COLS))
baseline_model.summary()

baseline_history = train_baseline_model(
    baseline_model,
    train_ds=train_ds,
    val_ds=val_ds,
    epochs=5,
    callbacks_list=None
)

"""### 18. Hyperparameter Tuning with Keras Tuner (Neural Network Search)

After establishing a baseline and understanding the dataset, the next step is to explore whether a deeper neural network can outperform the simple logistic-regression baseline. Rather than guessing network depth, units, or learning rate, we use **Keras Tuner** to perform structured hyperparameter optimization.

We define a `tuner_model_builder(hp)` function that exposes key architectural and training hyperparameters:

- **Dense layer width** (`32–256` units),
- **Number of hidden layers** (`1–3`),
- **Layer-specific units** for each additional layer,
- **Dropout regularization** to reduce overfitting,
- **Learning rate** (`1e-2`, `1e-3`, `1e-4`).

Using the **Hyperband** tuner:
- Faster exploration of large search spaces through adaptive resource allocation,
- Optimization objective: maximize **validation AUC**, appropriate for imbalanced binary classification,
- Early stopping prevents inefficient trials from running too long and helps stabilize the search results.

After searching across candidate architectures, we retrieve the best hyperparameters. These optimal settings will then guide the construction of a stronger, fully-trained model.

This step adds a systematic approach to model selection and avoids arbitrary architectural choices, improving both the performance and rigor of the final model.
"""

# ==========================================
# 9.5 Hyperparameter Tuning (Keras Tuner)
# ==========================================
try:
    import keras_tuner as kt
except ImportError:
    !pip install keras-tuner --quiet
    import keras_tuner as kt

import keras_tuner as kt

def tuner_model_builder(hp):

    model = tf.keras.Sequential()
    model.add(layers.Input(shape=(len(FEATURE_COLS),)))

    # Tune first Dense layer
    hp_units = hp.Int("units", min_value=32, max_value=256, step=32)
    model.add(layers.Dense(hp_units, activation="relu"))
    model.add(layers.Dropout(0.3))

    # Tune number of layers (1–3)
    for i in range(hp.Int("num_layers", 1, 3)):
        units_i = hp.Int(f"units_{i}", min_value=32, max_value=256, step=32)
        model.add(layers.Dense(units_i, activation="relu"))
        model.add(layers.Dropout(0.3))

    # Output layer
    model.add(layers.Dense(1, activation="sigmoid"))

    # Tune learning rate
    hp_lr = hp.Choice("learning_rate", [1e-2, 1e-3, 1e-4])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr),
        loss="binary_crossentropy",
        metrics=[
            tf.keras.metrics.AUC(name="auc"),
            tf.keras.metrics.Precision(name="precision"),
            tf.keras.metrics.Recall(name="recall")
        ]
    )
    return model


tuner = kt.Hyperband(
    tuner_model_builder,
    objective=kt.Objective("val_auc", direction="max"),
    max_epochs=5,
    factor=3,
    directory="tuner_logs",
    project_name="fraud_tuning"
)

# Early stopping to prevent bad trials from crashing
stop_early = tf.keras.callbacks.EarlyStopping(
    monitor="val_auc", patience=2, restore_best_weights=True
)

tuner.search(
    train_ds,
    validation_data=val_ds,
    epochs=5,
    callbacks=[stop_early]
)

best_hp = tuner.get_best_hyperparameters(1)[0]
print("Best Hyperparameters:", best_hp.values)

"""### 19. Handling Class Imbalance with Class Weights

Given the extreme class imbalance in the fraud detection dataset, directly training a neural network without corrective measures would bias the model toward predicting the majority (non-fraud) class. To address this, we compute **class weights** that penalize misclassification of the minority (fraud) class more heavily during training.

In this step, I use `sklearn.utils.compute_class_weight` with the `"balanced"` strategy to automatically compute weights based on the inverse frequency of each class in the **training set only**. This ensures that:
- Fraudulent transactions receive a higher training penalty,
- The model is encouraged to learn patterns associated with rare but critical fraud cases,
- Class imbalance is mitigated without altering the original data distribution through oversampling or undersampling.

The resulting class weight dictionary will be passed to `model.fit()` during training of the main neural network, enabling cost-sensitive learning while preserving the integrity of the dataset.
"""

# ========================================
# 10. Class Weights for Main Model
# ========================================


def compute_class_weights(y: pd.Series) -> Dict[int, float]:
    """
    Compute balanced class weights for imbalanced classification.

    Parameters
    ----------
    y : Series
        Target labels from the training dataset.

    Returns
    -------
    Dict[int, float]
        Mapping of class label -> computed class weight.
    """
    classes = np.unique(y)

    if len(classes) != 2:
        raise ValueError("Expected a binary classification problem (classes 0 and 1).")

    weights = compute_class_weight(
        class_weight="balanced",
        classes=classes,
        y=y
    )

    class_weights = {int(cls): float(w) for cls, w in zip(classes, weights)}

    print("\nComputed Class Weights:")
    for cls, w in class_weights.items():
        print(f"  Class {cls}: {w:.4f}")

    return class_weights


# -------- EXECUTION PIPELINE -------- #

class_weights = compute_class_weights(y_train)

"""### 20. Tuned Deep Neural Network Using Best Hyperparameters

After running hyperparameter tuning with Keras Tuner, we now construct a **final deep neural network (DNN)** that incorporates the best-performing hyperparameters found during the search.

From the tuner, we extract:
- The optimal number of units for the first dense layer (`units`),
- The optimal number of units for a second dense layer (`units_0`),
- The best **learning rate** for the Adam optimizer.

Using these values, we define `build_tuned_dnn(input_dim)`, which creates a DNN with:
- An input layer matching the number of preprocessed features,
- Two fully connected hidden layers with tuned widths and ReLU activations,
- **Batch Normalization** and **Dropout** (0.3) after each hidden layer to improve training stability and reduce overfitting,
- A single-unit sigmoid output layer for binary fraud probability predictions.

The model is compiled with:
- **Binary cross-entropy** loss,
- **Adam optimizer** using the tuned learning rate,
- Imbalance-aware metrics: AUC, Precision, and Recall.

This tuned DNN serves as the **main model** for the project. It leverages insights from hyperparameter optimization while adding regularization and normalization to better capture non-linear patterns in the highly imbalanced fraud detection task.
"""

# ============================================
# 10.5 DNN Model Using Best Hyperparameters
# ============================================

print("\nUsing Hyperparameters from Keras Tuner:\n")
print(best_hp.values)

# Extract best hyperparameters
units1 = best_hp.get("units")              # e.g., 128
units2 = best_hp.get("units_0")            # e.g., 32
learning_rate = best_hp.get("learning_rate")

# Build tuned DNN
def build_tuned_dnn(input_dim: int) -> tf.keras.Model:
    inputs = layers.Input(shape=(input_dim,), name="input_layer")

    # First layer from tuner
    x = layers.Dense(units1, activation="relu", name="dense_tuned_1")(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)

    # Second layer from tuner
    x = layers.Dense(units2, activation="relu", name="dense_tuned_2")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)

    outputs = layers.Dense(1, activation="sigmoid", name="output")(x)

    model = tf.keras.Model(inputs, outputs, name="tuned_dnn_model")

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss="binary_crossentropy",
        metrics=[
            tf.keras.metrics.AUC(name="auc"),
            tf.keras.metrics.Precision(name="precision"),
            tf.keras.metrics.Recall(name="recall")
        ]
    )

    return model


tuned_model = build_tuned_dnn(input_dim=len(FEATURE_COLS))
tuned_model.summary()

"""### 21. Training the Tuned Deep Neural Network

After defining the tuned DNN architecture, we train the model using the prepared TensorFlow input pipelines and the previously computed class weights.

Key aspects of this training step include:
- Training on the `train_ds` dataset and validating on `val_ds`,
- Applying **class weights** to penalize misclassification of the minority (fraud) class more heavily,
- Running a limited number of epochs to evaluate whether the tuned architecture improves validation performance over the baseline model.

This training run is treated as an **optional experimental step** rather than the final production training. Its goal is to assess whether hyperparameter tuning and increased model complexity yield measurable gains in AUC, precision, and recall under class-imbalanced conditions.

The results from this step help justify architectural choices and inform whether additional regularization, longer training, or early stopping should be applied in a final training run.
"""

# ============================================
# 10.6 Train Tuned Model (Optional Experiment)
# ============================================

print("\nTraining Tuned Model...\n")

tuned_history = tuned_model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10,              # short training is fine
    class_weight=class_weights,   # optional but recommended
    verbose=2
)

print("\nFinished training tuned model.\n")

"""### Model Selection Decision: Why the Tuned DNN Is Not Chosen as the Final Model

Although the tuned deep neural network achieves **very high training performance**, its **validation performance is unstable** and does not consistently improve over simpler models.

Several issues are evident from the training results:

- **Generalization gap**: Training AUC steadily increases, while validation AUC fluctuates between epochs, indicating signs of overfitting despite dropout and batch normalization.
- **Unstable precision–recall trade-off**: Validation precision remains low and highly variable across epochs, even when recall is high. This reflects a tendency to over-predict fraud, leading to an undesirable rate of false positives.
- **Inconsistent validation behavior**: Peaks in validation AUC and recall are not sustained across epochs, making model performance sensitive to training duration and threshold choice.
- **Limited benefit from added complexity**: Compared to the baseline model, the tuned DNN introduces significant architectural complexity without delivering a clear, stable improvement in validation performance.

Given the high cost of false positives in real-world fraud detection systems, **stability and reliable generalization** are more important than achieving near-perfect training metrics. As a result, this tuned DNN is treated as an exploratory experiment rather than the final deployed model.

The final model selection therefore prioritizes **simpler, more stable architectures** that generalize better to unseen data, even if they achieve slightly lower peak training performance.

### 22. Main Deep Neural Network for Fraud Detection

Based on the EDA, baseline results, and tuning experiments, we define a **main deep neural network (DNN)** that balances model capacity, regularization, and stability, without over-relying on aggressive hyperparameter tuning.

The `build_dnn_model()` function constructs a 3-layer fully connected network with:

- **Input layer**  
  Matches the dimensionality of the preprocessed feature vector (`input_dim`).

- **Hidden Block 1**  
  - Dense layer with `units1` (default: 128) and ReLU activation,  
  - Batch Normalization to stabilize training and mitigate internal covariate shift,  
  - Dropout (default: 30%) for regularization and overfitting control.

- **Hidden Block 2**  
  - Dense layer with `units2` (default: 64) and ReLU activation,  
  - Batch Normalization + Dropout as in the first block.

- **Hidden Block 3**  
  - Dense layer with `units3` (default: 32) and ReLU activation,  
  - No additional dropout here to preserve capacity before the output layer.

- **Output layer**  
  - Single-unit dense layer with a sigmoid activation to output fraud probability.

The model is compiled with:
- **Adam optimizer** (learning rate = 1e-3),
- **Binary cross-entropy** loss,
- Metrics tailored to imbalanced classification: AUC, Precision, and Recall.

Unlike the purely tuned architecture, this model uses a **fixed, well-regularized architecture** that performed more **consistently and stably** in validation, making it a strong candidate for the final fraud detection model.
"""

# ========================================
# 11. Main Deep Neural Network Model
# ========================================

def build_dnn_model(
    input_dim: int,
    hidden_units: Tuple[int, int, int] = (128, 64, 32),
    dropout_rate: float = 0.30,
    learning_rate: float = 1e-3
) -> tf.keras.Model:
    """
    Build the main fraud detection neural network.

    Parameters
    ----------
    input_dim : int
        Number of input features.
    hidden_units : Tuple[int, int, int]
        Number of units in each hidden layer.
    dropout_rate : float
        Dropout rate for regularization.
    learning_rate : float
        Adam optimizer learning rate.

    Returns
    -------
    tf.keras.Model
        A compiled TensorFlow DNN model.
    """
    units1, units2, units3 = hidden_units

    inputs = layers.Input(shape=(input_dim,), name="input_layer")

    # Block 1
    x = layers.Dense(units1, activation="relu", name="dense_1")(inputs)
    x = layers.BatchNormalization(name="bn_1")(x)
    x = layers.Dropout(dropout_rate, name="dropout_1")(x)

    # Block 2
    x = layers.Dense(units2, activation="relu", name="dense_2")(x)
    x = layers.BatchNormalization(name="bn_2")(x)
    x = layers.Dropout(dropout_rate, name="dropout_2")(x)

    # Block 3
    x = layers.Dense(units3, activation="relu", name="dense_3")(x)

    # Output layer (binary classification)
    outputs = layers.Dense(1, activation="sigmoid", name="output_layer")(x)

    model = models.Model(
        inputs=inputs,
        outputs=outputs,
        name="fraud_dnn_model"
    )

    # Compile using TF best practices
    model.compile(
        optimizer=optimizers.Adam(learning_rate=learning_rate),
        loss=losses.BinaryCrossentropy(),
        metrics=[
            metrics.AUC(name="auc"),
            metrics.Precision(name="precision"),
            metrics.Recall(name="recall"),
        ]
    )

    return model


# -------- EXECUTION PIPELINE -------- #

model = build_dnn_model(input_dim=len(FEATURE_COLS))
model.summary()

"""### 23. Training Callbacks and Experiment Tracking with TensorBoard

To train the final model robustly and transparently, we integrate **Keras callbacks** that support experiment tracking, regularization, and adaptive optimization.

In this section, we configure and apply three key callbacks:

- **TensorBoard logging**  
  A timestamped log directory is created for each training run, enabling detailed visualization of training and validation metrics, loss curves, and model behavior over time. This supports experiment comparison and reproducibility.

- **EarlyStopping**  
  Monitors validation AUC and stops training if performance stops improving. By restoring the best weights, this callback prevents overfitting and ensures that the final model corresponds to the strongest validation performance.

- **ReduceLROnPlateau**  
  Automatically reduces the learning rate when validation AUC plateaus, allowing the optimizer to make finer updates and improving convergence stability during later training stages.

The `train_main_model()` function wraps the training logic and applies class weights to handle severe class imbalance. The model is allowed to train for up to 50 epochs, but early stopping typically halts training sooner, balancing performance and efficiency.

Together, these callbacks promote **stable training, reproducibility, and rigorous experiment tracking**, aligning the training process with production-oriented TensorFlow best practices.
"""

# ========================================
# 12. Training Callbacks & TensorBoard
# ========================================

def create_log_dir(base_dir: Path = LOGS_DIR) -> Path:
    """
    Create a timestamped logging directory for TensorBoard.

    Parameters
    ----------
    base_dir : Path
        Base directory where logs/ will be stored.

    Returns
    -------
    Path
        Full path to the created log directory.
    """
    timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    log_path = base_dir / timestamp
    log_path.mkdir(parents=True, exist_ok=True)
    print(f"TensorBoard logs → {log_path.resolve()}")
    return log_path


def create_callbacks(log_dir: Path) -> List[callbacks.Callback]:
    """
    Create a list of Keras callbacks including TensorBoard,
    EarlyStopping, and ReduceLROnPlateau.

    Parameters
    ----------
    log_dir : Path
        Directory where TensorBoard logs will be saved.

    Returns
    -------
    List[Callback]
        List of configured Keras callbacks.
    """
    return [
        callbacks.TensorBoard(
            log_dir=str(log_dir),
            histogram_freq=1,
            write_graph=True,
            write_images=False,
        ),
        callbacks.EarlyStopping(
            monitor="val_auc",
            mode="max",
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        callbacks.ReduceLROnPlateau(
            monitor="val_auc",
            mode="max",
            factor=0.5,
            patience=3,
            verbose=1
        ),
    ]


def train_main_model(
    model: tf.keras.Model,
    train_ds: tf.data.Dataset,
    val_ds: tf.data.Dataset,
    class_weights: Optional[dict] = None,
    epochs: int = 50,
    callbacks_list: Optional[List[callbacks.Callback]] = None
) -> tf.keras.callbacks.History:
    """
    Train the main fraud detection model with callbacks.

    Parameters
    ----------
    model : tf.keras.Model
        The compiled main DNN model.
    train_ds : tf.data.Dataset
        Training dataset.
    val_ds : tf.data.Dataset
        Validation dataset.
    class_weights : dict, optional
        Class weight mapping for handling imbalance.
    epochs : int
        Maximum number of training epochs.
    callbacks_list : list, optional
        List of Keras callbacks.

    Returns
    -------
    History
        History object containing training curves.
    """
    print("\n🚀 Starting model training...\n")

    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        class_weight=class_weights,
        callbacks=callbacks_list,
        verbose=2
    )

    print("\n🏁 Training complete.")
    return history


# -------- EXECUTION PIPELINE -------- #

log_dir = create_log_dir()
callback_list = create_callbacks(log_dir)

history = train_main_model(
    model=model,
    train_ds=train_ds,
    val_ds=val_ds,
    class_weights=class_weights,
    epochs=50,
    callbacks_list=callback_list
)

"""### Training Summary – Main Deep Neural Network

The main DNN model shows **strong and stable performance** on the validation set:

- **Training AUC** steadily increases and remains high (> 0.99), while  
  **validation AUC** peaks around **0.98** (best at epoch 10), indicating
  good generalization rather than extreme overfitting.
- **Recall on the validation set** stays consistently high (around **0.75–0.80**),
  meaning the model successfully identifies a large proportion of fraudulent
  transactions.
- **Precision remains modest** (~0.12–0.17), which is expected in a severely
  imbalanced setting with class weights that emphasize recall. This reflects
  a trade-off: the model is tuned to catch more fraud at the cost of some
  false positives.

The callbacks behaved as intended:
- **ReduceLROnPlateau** lowered the learning rate after the validation AUC
  plateaued, helping stabilize training.
- **EarlyStopping** stopped training at epoch 15 and restored the weights from
  the best epoch (epoch 10), ensuring the final model corresponds to the
  highest observed validation AUC.

Overall, this model provides a **good balance between high AUC, strong recall,
and training stability**, making it a suitable choice as the **final fraud
detection model** for this project.

### 25. Evaluation on the Test Set and Threshold Tuning

Once the final model is trained and validated, it must be evaluated on the **unseen test set** to estimate real-world performance. In fraud detection, the default decision threshold of 0.50 is rarely optimal because of severe class imbalance. Therefore, this section performs two key evaluation steps:

1. **Predicting fraud probabilities on the test set**  
   Using the trained model, we generate probability scores for each transaction. AUC (Area Under the ROC Curve) is computed to measure the model’s ability to rank fraudulent transactions above legitimate ones, independent of any specific threshold.

2. **Threshold tuning using the F1-score**  
   Since fraud prediction requires a balance between *precision* (avoiding false alarms) and *recall* (catching true fraud), we compute the precision–recall curve on the test probabilities.  
   For each possible threshold, the F1-score is calculated as the harmonic mean of precision and recall.  
   This allows us to:
   - Identify the **best decision threshold** for classification,
   - Quantify the trade-offs between precision and recall,
   - Improve model usefulness beyond naïve 0.50 cutoff.

The result is an optimized threshold that maximizes detection effectiveness while acknowledging the high cost and rarity of fraud cases. This step transforms the model from a probability predictor into a practically applicable fraud-classification system.
"""

# ========================================
# 13. Evaluation on Test Set + Threshold Tuning
# ========================================

def predict_probabilities(
    model: tf.keras.Model,
    X_test: pd.DataFrame,
    batch_size: int = 2048
) -> np.ndarray:
    """
    Predict fraud probabilities for the test set.

    Parameters
    ----------
    model : tf.keras.Model
        Trained model.
    X_test : DataFrame
        Scaled test features.
    batch_size : int
        Batch size for inference.

    Returns
    -------
    np.ndarray
        Array of predicted probabilities.
    """
    X_array = X_test.values.astype("float32")
    probs = model.predict(X_array, batch_size=batch_size).ravel()
    return probs


def compute_auc(y_true: np.ndarray, y_prob: np.ndarray) -> float:
    """Compute AUC score."""
    return roc_auc_score(y_true, y_prob)


def tune_threshold_f1(
    y_true: np.ndarray,
    y_prob: np.ndarray
) -> Dict[str, float]:
    """
    Tune probability threshold to maximize F1-score.

    Parameters
    ----------
    y_true : ndarray
        True labels.
    y_prob : ndarray
        Predicted probabilities.

    Returns
    -------
    dict
        Best threshold, best F1, precision array, recall array.
    """
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)

    best_idx = f1_scores.argmax()

    best_threshold = thresholds[best_idx]
    best_f1 = f1_scores[best_idx]

    print("\nOptimal Threshold Tuning:")
    print(f"  Best Threshold:     {best_threshold:.4f}")
    print(f"  Best F1-Score:      {best_f1:.4f}")
    print(f"  Precision @ best:   {precisions[best_idx]:.4f}")
    print(f"  Recall @ best:      {recalls[best_idx]:.4f}")

    return {
        "best_threshold": float(best_threshold),
        "best_f1": float(best_f1),
        "precisions": precisions,
        "recalls": recalls,
        "thresholds": thresholds,
    }


# -------- EXECUTION PIPELINE -------- #

# 1. Predict probabilities
y_test_array = y_test.values.astype("int32")
y_probabilities = predict_probabilities(model, X_test_scaled)

# 2. AUC evaluation
test_auc = compute_auc(y_test_array, y_probabilities)
print(f"\nTest AUC: {test_auc:.6f}")

# 3. Threshold tuning via F1-score
threshold_results = tune_threshold_f1(y_test_array, y_probabilities)

best_threshold = threshold_results["best_threshold"]
best_f1 = threshold_results["best_f1"]

"""### 26. Final Classification Report and Confusion Matrix

After tuning the decision threshold for optimal F1-score, we generate the final binary fraud predictions on the test set and evaluate them using two key tools:

1. **Classification Report**  
   Provides precision, recall, F1-score, and support for both classes.  
   These metrics offer a detailed view of how well the model distinguishes between fraudulent and legitimate transactions under the optimized threshold.

2. **Confusion Matrix**  
   Summarizes:
   - True Positives (correctly identified fraud),
   - True Negatives (correctly identified non-fraud),
   - False Positives (legitimate transactions incorrectly flagged),
   - False Negatives (fraud missed).  
   
   This matrix is especially important in fraud detection, where the costs of false positives (customer inconvenience) and false negatives (financial loss) differ significantly. Evaluating both helps determine whether the model’s behavior aligns with domain priorities.

Together, the classification report and confusion matrix translate model probabilities into *actionable performance insights*, completing the evaluation pipeline and providing a holistic picture of real-world predictive behavior.
"""

# ========================================
# 14. Classification Report & Confusion Matrix
# ========================================

def evaluate_classification(
    y_true: np.ndarray,
    y_pred: np.ndarray
) -> Dict[str, np.ndarray]:
    """
    Compute and print the classification report and confusion matrix.

    Parameters
    ----------
    y_true : ndarray
        True binary labels.
    y_pred : ndarray
        Binary predictions using an optimized classification threshold.

    Returns
    -------
    dict
        Dictionary containing:
        - report (str)
        - confusion_matrix (ndarray)
    """
    print("\n=== Classification Report ===\n")
    report = classification_report(y_true, y_pred, digits=4)
    print(report)

    cm = confusion_matrix(y_true, y_pred)
    print("=== Confusion Matrix ===")
    print(cm)

    return {
        "classification_report": report,
        "confusion_matrix": cm
    }


# -------- EXECUTION PIPELINE -------- #

y_pred_optimal = (y_probabilities >= best_threshold).astype(int)

classification_results = evaluate_classification(
    y_true=y_test_array,
    y_pred=y_pred_optimal
)

"""### Final Test Set Performance Interpretation

The final evaluation results demonstrate that the selected model performs **very strongly on unseen data**, while maintaining a realistic balance between fraud detection and false alarms.

#### Key observations from the classification report:
- **Fraud detection performance (Class = 1)**:
  - **Recall = 0.80** → The model successfully detects **80% of fraudulent transactions**, missing only a small fraction.
  - **Precision = 0.87** → When the model flags a transaction as fraud, it is correct **nearly 87% of the time**, indicating a low false-alarm rate.
  - **F1-score = 0.83** → Shows a well-balanced trade-off between recall and precision under the optimized decision threshold.

- **Non-fraud performance (Class = 0)**:
  - Precision and recall are both **> 99.9%**, confirming that the model rarely disrupts legitimate transactions.

#### Confusion matrix analysis:
- **True Positives (Fraud correctly detected)**: 20  
- **False Negatives (Fraud missed)**: 5  
- **False Positives (Legitimate flagged as fraud)**: 3  
- **True Negatives**: 14,950  

This behavior is highly desirable for fraud detection:
- Only **5 fraud cases missed**, limiting financial and reputational risk.
- Only **3 legitimate transactions incorrectly flagged**, minimizing customer friction and false alarms.

#### Overall assessment:
The model achieves **excellent generalization performance** with:
- Very high discrimination ability (consistent with strong test AUC),
- A carefully tuned decision threshold,
- A practical balance between catching fraud and avoiding unnecessary interruptions to legitimate users.

These results validate the final model choice and confirm that the training, validation, imbalance handling, and threshold tuning strategies were effective. The system is well-suited as a **risk-scoring or decision-support component** in a real-world fraud detection pipeline.

### 27. Training Dynamics Visualization (Learning Curves)

To better understand how the model learns over time and to diagnose potential issues such as overfitting or underfitting, we visualize the **training and validation curves** for key metrics.

Using the training `History` object returned by Keras, we plot:
- **Loss** – to observe convergence behavior and optimization stability,
- **AUC** – to evaluate how well the model separates fraud from non-fraud over epochs,
- **Precision** – to monitor false-positive behavior,
- **Recall** – to track how effectively fraud cases are identified.

Each plot displays both **training** and **validation** curves, enabling direct comparison across epochs. This helps verify that:
- Training progresses smoothly without divergence,
- Validation performance improves and stabilizes,
- Callbacks such as EarlyStopping and ReduceLROnPlateau are behaving as expected.

Visualizing these learning curves provides a final diagnostic check, confirming that the selected model generalizes well and that the training process is stable and well-regularized.
"""

# ========================================
# 15. Training Curves (Loss, AUC, Precision, Recall)
# ========================================

def plot_training_curves(history: tf.keras.callbacks.History,
                         metrics: List[str] = ["loss", "auc", "precision", "recall"]) -> None:
    """
    Plot training & validation curves for the specified metrics.

    Parameters
    ----------
    history : History
        Keras training history object.
    metrics : List[str]
        Metrics to plot. Automatically adds 'val_' prefix for validation.

    Returns
    -------
    None
    """
    history_df = pd.DataFrame(history.history)

    num_metrics = len(metrics)
    rows = (num_metrics + 1) // 2  # arrange plots in grid
    plt.figure(figsize=(14, 4 * rows))

    for idx, metric in enumerate(metrics, 1):
        val_metric = f"val_{metric}"

        if metric not in history_df or val_metric not in history_df:
            print(f"⚠ Warning: Metric '{metric}' not found in history. Skipping.")
            continue

        plt.subplot(rows, 2, idx)
        plt.plot(history_df[metric], label=f"Train {metric}")
        plt.plot(history_df[val_metric], label=f"Val {metric}")

        plt.title(metric.capitalize())
        plt.xlabel("Epoch")
        plt.ylabel(metric.capitalize())
        plt.legend()

    plt.tight_layout()
    plt.show()


# -------- EXECUTION PIPELINE -------- #

plot_training_curves(history)

"""### Interpretation of Training Curves

The learning curves provide strong evidence that the final DNN model is **well-trained, stable, and properly regularized**.

#### Loss
- Both **training and validation loss decrease smoothly**, with no divergence.
- Validation loss closely tracks training loss and stabilizes after ~10 epochs.
- This indicates good convergence and **no severe overfitting**.

#### AUC
- Training AUC rapidly increases and remains close to 1.0.
- Validation AUC rises steadily and plateaus around **0.97–0.98**, matching the best validation epochs selected by EarlyStopping.
- The small, consistent gap between training and validation AUC suggests **strong generalization performance**.

#### Precision
- Training precision increases gradually as the model learns more discriminative fraud patterns.
- Validation precision remains **moderate but stable**, which is expected given:
  - The extreme class imbalance,
  - The use of class weights favoring recall.
- Fluctuations are normal due to the small number of fraud cases in validation data.

#### Recall
- Training recall increases steadily and remains high (>0.95).
- Validation recall stays around **0.75–0.80**, indicating the model reliably detects most fraud cases across epochs.
- Importantly, recall does not collapse as training progresses, confirming that regularization and callbacks are effective.

#### Overall assessment
The curves collectively show:
- Stable optimization,
- No uncontrolled overfitting,
- A deliberate precision–recall trade-off suitable for fraud detection,
- Proper functioning of EarlyStopping and learning-rate scheduling.

These results confirm that the chosen model achieves **strong generalization and consistent behavior**, justifying its selection as the final fraud detection model.

### 28. ROC and Precision–Recall Curves on the Test Set

To complement the scalar metrics (AUC, precision, recall, F1-score), we visualize the model’s ranking and classification behavior using **ROC** and **Precision–Recall (PR)** curves on the test set.

- **ROC Curve (Receiver Operating Characteristic)**  
  Plots the True Positive Rate vs. False Positive Rate across all possible thresholds.  
  The ROC curve and its corresponding AUC summarize how well the model can **separate fraudulent from non-fraudulent transactions**, independent of any specific decision threshold.

- **Precision–Recall Curve**  
  Plots precision vs. recall for every possible threshold.  
  This curve is particularly informative for **highly imbalanced problems** like fraud detection, where the positive class is rare and precision/recall trade-offs matter more than overall accuracy.

By examining both curves side by side, we can:
- Verify that the model maintains strong performance across a wide range of thresholds,
- Confirm that the tuned operating point (chosen via F1 optimization) lies in a region with **good precision–recall balance**,
- Provide a more complete and visually interpretable assessment of model behavior on the test set.
"""

# ========================================
# 16. ROC & Precision-Recall Curves
# ========================================

def plot_roc_pr_curves(
    y_true: np.ndarray,
    y_prob: np.ndarray,
    figsize: tuple = (14, 5)
) -> None:
    """
    Plot the ROC curve and Precision-Recall curve side by side.

    Parameters
    ----------
    y_true : ndarray
        Ground truth labels.
    y_prob : ndarray
        Predicted probabilities from the model.
    figsize : tuple
        Figure size for the combined plots.

    Returns
    -------
    None
    """
    fig, ax = plt.subplots(1, 2, figsize=figsize)

    # ROC Curve
    RocCurveDisplay.from_predictions(
        y_true=y_true,
        y_pred=y_prob,
        ax=ax[0]
    )
    ax[0].set_title("ROC Curve (Test Set)")
    ax[0].grid(alpha=0.3)

    # Precision–Recall Curve
    PrecisionRecallDisplay.from_predictions(
        y_true=y_true,
        y_pred=y_prob,
        ax=ax[1]
    )
    ax[1].set_title("Precision-Recall Curve (Test Set)")
    ax[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.show()


# -------- EXECUTION PIPELINE -------- #

plot_roc_pr_curves(y_true=y_test_array, y_prob=y_probabilities)

"""### Interpretation of ROC and Precision–Recall Curves

The ROC and Precision–Recall curves on the test set provide strong visual confirmation of the model’s effectiveness in a highly imbalanced fraud detection setting.

#### ROC Curve
- The ROC curve rises sharply toward the top-left corner, with a **test AUC of approximately 0.99**.
- This indicates excellent discriminative ability: the model is highly effective at ranking fraudulent transactions above legitimate ones across a wide range of thresholds.
- Such a high AUC confirms that the model captures meaningful patterns rather than relying on chance or majority-class bias.

#### Precision–Recall Curve
- The Precision–Recall curve shows strong performance in the high-recall region, with an **Average Precision (AP) of approximately 0.76**.
- Given the extreme rarity of fraud in the dataset, this represents a substantial improvement over random guessing.
- The curve illustrates the expected trade-off: precision gradually decreases as recall approaches 1.0, reflecting the increasing number of false positives when attempting to capture nearly all fraud cases.

#### Overall assessment
Taken together:
- The ROC curve demonstrates **excellent ranking performance**,
- The Precision–Recall curve confirms that the model maintains **meaningful precision at useful recall levels**,
- The chosen decision threshold (optimized via F1-score) lies in a region that balances fraud detection effectiveness with manageable false-positive rates.

These curves validate the final model as a strong and reliable fraud detection system, well-suited for real-world deployment where probability scoring and threshold tuning are critical.

### 29. Model Persistence and Export for Deployment

To complete the end-to-end machine learning pipeline, we persist the trained fraud detection model in formats suitable for reuse, evaluation, and deployment.

Two complementary export formats are used:

1. **Native Keras format (`.keras`)**  
   - Saved using the TensorFlow 2.x recommended standard,
   - Preserves model architecture and trained weights,
   - Optimized for easy reloading, experimentation, and reproducibility.

2. **TensorFlow SavedModel format**  
   - Exported in a serving-compatible structure,
   - Suitable for production use cases such as:
     - TensorFlow Serving,
     - Conversion to TensorFlow Lite,
     - ONNX export for cross-platform inference.

A reload sanity check is performed after saving the `.keras` model to ensure that the model can be restored correctly without errors. This step confirms that the trained model is **portable, reusable, and deployment-ready**.

By persisting both formats, the project transitions from model experimentation to **production-oriented machine learning**.
"""

# ========================================
# 17. Save Model (Keras + SavedModel)
# ========================================

def save_keras_model(model: tf.keras.Model, model_dir: Path = MODELS_DIR,
                     filename: str = "fraud_model.keras") -> Path:
    """
    Save a Keras model in .keras format (recommended TF 2.x standard).

    Parameters
    ----------
    model : tf.keras.Model
        Trained Keras model.
    model_dir : Path
        Directory where the model will be saved.
    filename : str
        File name for the exported .keras model.

    Returns
    -------
    Path
        Full path of the saved model file.
    """
    model_dir.mkdir(parents=True, exist_ok=True)
    save_path = model_dir / filename

    model.save(save_path, include_optimizer=False)
    print(f"Saved Keras model → {save_path.resolve()}")

    return save_path


def export_saved_model(model: tf.keras.Model, export_dir: Path = MODELS_DIR,
                       foldername: str = "saved_model_tfserving") -> Path:
    """
    Export the model in TensorFlow SavedModel format.
    This version is suitable for:
        - TensorFlow Serving
        - TFLite conversion
        - ONNX export

    Parameters
    ----------
    model : tf.keras.Model
        Loaded or trained model.
    export_dir : Path
        Directory where SavedModel will be stored.
    foldername : str
        Subfolder for the SavedModel export.

    Returns
    -------
    Path
        Full path of the SavedModel export directory.
    """
    export_path = export_dir / foldername
    export_path.mkdir(parents=True, exist_ok=True)

    model.export(export_path)
    print(f"Exported SavedModel → {export_path.resolve()}")

    return export_path


# -------- EXECUTION PIPELINE -------- #

# Save model in Keras format
keras_model_path = save_keras_model(model)

# Reload (sanity check)
loaded_model = tf.keras.models.load_model(keras_model_path)
print("Model reloaded successfully for sanity check.")

# Export SavedModel
saved_model_path = export_saved_model(loaded_model)

"""### 30. Monitoring and Prediction Drift Simulation

To extend the project toward **MLOps and production monitoring**, we simulate how the model would behave in a deployed setting and compute statistics that could be used to detect **prediction drift** over time.

This monitoring step includes:

1. **Simulated production batch**
   - We randomly sample a batch of 500 transactions from the scaled test set using `simulate_production_batch`.
   - This batch mimics a “live” stream of incoming transactions in production.

2. **Batch inference**
   - Using the deployed model (`loaded_model`), we generate predicted fraud probabilities for the new batch via `predict_batch_probabilities`.
   - These predictions are used for both labeled and label-free monitoring.

3. **Probability distribution visualization**
   - We plot a histogram of the predicted probabilities for the new batch.
   - In a real system, this distribution would be compared over time; large shifts (e.g., suddenly many more high-risk scores) can indicate **data or prediction drift**.

4. **Batch-level AUC (when labels are available)**
   - For this simulated scenario, we still have access to true labels, so we compute batch AUC using TensorFlow’s `AUC` metric.
   - This acts as a **sanity check**: if batch AUC drops over time, it may signal model degradation.

5. **Label-free monitoring statistics**
   - Since true labels often arrive late or are partially available in production, we also compute **label-free statistics** on the predicted probabilities:
     - Mean, standard deviation, min, max,
     - **High-risk ratio**: proportion of predictions above the chosen fraud threshold.
   - These summary statistics can be tracked over time to detect **prediction drift** even when ground truth labels are not yet known.

By adding this monitoring simulation, the project goes beyond pure model training and evaluation, demonstrating how the fraud detection model could be **observed and validated continuously** in a real-world deployment pipeline.

"""

# ========================================
# 18. Monitoring / Prediction Drift Simulation
# ========================================

def simulate_production_batch(
    X_source: pd.DataFrame,
    y_source: pd.Series,
    batch_size: int = 500,
    seed: int = 42
) -> Tuple[pd.DataFrame, np.ndarray]:
    """
    Sample a batch of data simulating production inference.

    Parameters
    ----------
    X_source : DataFrame
        Scaled feature dataset (e.g., X_test_scaled).
    y_source : Series
        True labels aligned with X_source index.
    batch_size : int
        Number of samples to pull for simulation.
    seed : int
        Random seed.

    Returns
    -------
    new_batch : DataFrame
        Sampled feature batch.
    true_labels : ndarray
        Corresponding true labels.
    """
    new_batch = X_source.sample(batch_size, random_state=seed)
    true_labels = y_source.loc[new_batch.index].values
    return new_batch, true_labels


def predict_batch_probabilities(
    model: tf.keras.Model,
    batch: pd.DataFrame,
    batch_infer_size: int = 2048
) -> np.ndarray:
    """
    Predict probabilities for a batch (TensorFlow-first inference).

    Parameters
    ----------
    model : tf.keras.Model
        Loaded or trained model.
    batch : DataFrame
        Batch of features.
    batch_infer_size : int
        Batch size for TensorFlow prediction.

    Returns
    -------
    ndarray
        Predicted fraud probabilities.
    """
    return model.predict(
        batch.values.astype("float32"),
        batch_size=batch_infer_size,
        verbose=0
    ).ravel()


def compute_batch_auc_tf(
    y_true: np.ndarray,
    y_prob: np.ndarray
) -> Optional[float]:
    """
    Compute AUC using TensorFlow metrics (if both classes present).

    Returns None if AUC is undefined.
    """
    if len(np.unique(y_true)) <= 1:
        return None

    auc = tf.keras.metrics.AUC()
    auc.update_state(y_true.astype("int32"), y_prob)
    return float(auc.result().numpy())


def compute_probability_statistics(
    y_prob: np.ndarray,
    threshold: float
) -> Dict[str, float]:
    """
    Compute drift-friendly monitoring statistics that do NOT require labels.

    Examples:
        - Mean/Std probability
        - Min/Max probability
        - High-risk prediction ratio
    """
    return {
        "prob_mean": float(np.mean(y_prob)),
        "prob_std": float(np.std(y_prob)),
        "prob_min": float(np.min(y_prob)),
        "prob_max": float(np.max(y_prob)),
        "high_risk_ratio": float(np.mean(y_prob >= threshold)),
        "batch_size": len(y_prob)
    }


def plot_probability_distribution(y_prob: np.ndarray) -> None:
    """Plot the histogram of predicted probabilities."""
    plt.figure(figsize=(7, 4))
    plt.hist(y_prob, bins=30, edgecolor="black")
    plt.title("Distribution of Predicted Fraud Probabilities (New Batch)")
    plt.xlabel("Predicted probability")
    plt.ylabel("Count")
    plt.grid(alpha=0.3)
    plt.show()


# -------- EXECUTION PIPELINE -------- #

# 1. Simulate production batch
new_batch, true_labels_batch = simulate_production_batch(
    X_test_scaled,
    y_test,
    batch_size=500,
    seed=SEED
)

# 2. Model inference
batch_probs = predict_batch_probabilities(loaded_model, new_batch)

# 3. Plot probability distribution (visual drift check)
plot_probability_distribution(batch_probs)

# 4. Compute labeled metric (AUC) if possible
batch_auc = compute_batch_auc_tf(true_labels_batch, batch_probs)
if batch_auc is not None:
    print(f"AUC on simulated production batch: {batch_auc:.4f}")
else:
    print("AUC on simulated batch: undefined (only one class present).")

# 5. Compute drift-related monitoring metrics (label-free)
monitor_stats = compute_probability_statistics(batch_probs, threshold=best_threshold)

print("\nMonitoring Statistics:")
for key, value in monitor_stats.items():
    print(f"  {key.replace('_',' ').title()}: {value:.4f}")

"""### Interpretation of Drift Simulation and Monitoring Statistics

In this simulated production batch of 500 transactions, the **AUC is reported as undefined** because the batch happens to contain only one class (either all legitimate or effectively no fraud cases). This situation is entirely realistic in production fraud detection, where fraud is extremely rare and short time windows (e.g., hourly batches) may easily contain **zero or very few fraud labels**. It also highlights why relying solely on labeled metrics for real-time monitoring is risky—labels may be delayed, sparse, or unavailable.

The label-free monitoring statistics provide a more robust picture of the model’s behavior on this batch:

- **Mean predicted probability ≈ 0.0412** and **standard deviation ≈ 0.0844**  
  → Most transactions are scored as low-risk, with moderate variability, which aligns with expectations in a population dominated by legitimate transactions.

- **Minimum probability ≈ 0.0001** and **maximum probability ≈ 0.7417**  
  → The model produces a wide range of scores, but very few predictions are near 1.0, suggesting that only a small subset of transactions are considered suspicious.

- **High-risk ratio = 0.0000 (for the tuned threshold)**  
  → Under the current decision threshold, **none** of the 500 transactions cross the fraud-alert threshold in this particular batch.  
  On its own, this is not necessarily problematic—small batches can legitimately contain no high-risk cases—but if this pattern persisted across many batches, it might signal:
  - A genuine drop in fraud prevalence, or  
  - A potential **prediction drift** or **threshold miscalibration** issue.

These statistics illustrate how, in a real deployment, we could:
- Track probability distributions and high-risk ratios over time,
- Compare them against historical baselines,
- Trigger alerts if the model suddenly becomes overly conservative or overly aggressive.

Overall, this simulation demonstrates that the monitoring pipeline can still provide **useful, label-free signals** even when traditional evaluation metrics like AUC are not defined for a given batch.
"""

