# -*- coding: utf-8 -*-
"""CreditCardFraudDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SdJdYyCkP71SX-WAIYL46JwKgqkpuhHa
"""

# ==============================
# 1. Project Setup & Imports
# ==============================

import datetime
import os
from pathlib import Path

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

!pip install opendatasets --quiet
import opendatasets as od
import tensorflow as tf
from sklearn.metrics import (PrecisionRecallDisplay, RocCurveDisplay,
                             classification_report, confusion_matrix,
                             precision_recall_curve, roc_auc_score)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import class_weight
from tensorflow.keras import (callbacks, layers, losses, metrics, models,
                              optimizers)

# -----------------------
# Global configuration
# -----------------------

SEED: int = 42
PROJECT_ROOT = Path("creditcard-fraud-mlops")
DATA_DIR = PROJECT_ROOT / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
MODELS_DIR = PROJECT_ROOT / "models"
NOTEBOOKS_DIR = PROJECT_ROOT / "notebooks"
LOGS_DIR = PROJECT_ROOT / "logs"


def set_global_seed(seed: int = 42) -> None:
    """Set random seeds for reproducibility."""
    np.random.seed(seed)
    tf.random.set_seed(seed)


def create_project_structure(root: Path = PROJECT_ROOT) -> None:
    """
    Create standardized project folder structure.

    Parameters
    ----------
    root : Path
        Root directory for the project (default: 'creditcard-fraud-mlops').
    """
    subdirs = [
        RAW_DATA_DIR,
        PROCESSED_DATA_DIR,
        MODELS_DIR,
        NOTEBOOKS_DIR,
        LOGS_DIR,
    ]
    for d in subdirs:
        d.mkdir(parents=True, exist_ok=True)


# -----------------------
# Initialize environment
# -----------------------

set_global_seed(SEED)
create_project_structure()

# Change working directory to project root (for Colab / notebooks)
os.chdir(PROJECT_ROOT)

print(f"Working directory: {Path.cwd()}")
print(f"Data directory:    {DATA_DIR.resolve()}")
print(f"Models directory:  {MODELS_DIR.resolve()}")

# ========================================
# 2. Download & Load Credit Card Dataset
# ========================================

from pathlib import Path

import opendatasets as od
import pandas as pd


def download_creditcard_dataset(url: str, download_dir: Path) -> Path:
    """
    Download the Kaggle credit card fraud dataset if not already present.

    Parameters
    ----------
    url : str
        Kaggle dataset URL.
    download_dir : Path
        Local directory where the dataset will be downloaded.

    Returns
    -------
    Path
        Path to the downloaded CSV file.
    """
    download_dir.mkdir(parents=True, exist_ok=True)

    # Only download if not already present
    csv_path = download_dir / "creditcardfraud" / "creditcard.csv"
    if not csv_path.exists():
        print("Downloading dataset...")
        od.download(url, str(download_dir))
    else:
        print("Dataset already exists. Skipping download.")

    return csv_path


def load_creditcard_dataframe(csv_path: Path) -> pd.DataFrame:
    """
    Load the credit card fraud dataset CSV into a pandas DataFrame.

    Parameters
    ----------
    csv_path : Path
        Path to the dataset CSV.

    Returns
    -------
    DataFrame
        Loaded dataset.
    """
    if not csv_path.exists():
        raise FileNotFoundError(f"CSV not found at: {csv_path}")

    df = pd.read_csv(csv_path)
    print(f"Loaded dataset with shape: {df.shape}")
    return df


# -------- Execute the workflow -------- #

DATA_URL = "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
csv_path = download_creditcard_dataset(DATA_URL, RAW_DATA_DIR)

df = load_creditcard_dataframe(csv_path)

# Quick overview
print("\nDataset Info:")
print(df.info())
df.head()

"""### Dataset Description (Credit Card Fraud Detection)

- **Source**: Kaggle â€“ â€œCredit Card Fraud Detectionâ€ (European cardholder transactions, September 2013).
- **Rows**: 284,807 transactions.
- **Columns**:
  - `Time`: seconds elapsed between each transaction and the first transaction in the dataset.
  - `V1`â€“`V28`: numeric features resulting from a PCA transformation applied to the original, confidential cardholder features (to protect privacy).
  - `Amount`: transaction amount.
  - `Class`: target label (0 = legitimate transaction, 1 = fraudulent transaction).

The dataset is **highly imbalanced**: only about 0.17% of transactions are fraudulent. This reflects real-world fraud detection settings, where most transactions are normal and fraud is rare but **high-impact**.

This dataset is relevant because:
- It models a **real, high-stakes** financial security problem.
- The cost of missing fraud (false negatives) is high for banks and customers.
- The cost of flagging legitimate transactions (false positives) is also important because it hurts user experience and trust.

In this project, we build an **end-to-end TensorFlow pipeline** to detect fraudulent transactions and handle **class imbalance**, **model evaluation**, and **deployment readiness**.
"""

# ========================================
# 4. Subsample for Faster Experimentation
# ========================================
from pathlib import Path

import pandas as pd


def stratified_subsample(
    df: pd.DataFrame,
    target_col: str = "Class",
    subset_size: int = 100_000,
    seed: int = 42,
) -> pd.DataFrame:
    """
    Create a stratified subset of the dataset while preserving the class ratio.

    Parameters
    ----------
    df : DataFrame
        Full credit card fraud dataset.
    target_col : str
        Name of the target label column (default "Class").
    subset_size : int
        Desired number of rows in the subsampled dataset.
    seed : int
        Random seed for reproducibility.

    Returns
    -------
    DataFrame
        Stratified and shuffled subsampled DataFrame.
    """
    # Separate fraud vs non-fraud
    df_pos = df[df[target_col] == 1]
    df_neg = df[df[target_col] == 0]

    # Number of samples proportional to original class ratio
    n_pos = int(subset_size * len(df_pos) / len(df))
    n_neg = subset_size - n_pos

    print(f"Sampling {n_pos} fraud and {n_neg} non-fraud rows...")

    pos_sample = df_pos.sample(n=n_pos, random_state=seed)
    neg_sample = df_neg.sample(n=n_neg, random_state=seed)

    # Combine and shuffle
    subset = (
        pd.concat([pos_sample, neg_sample])
        .sample(frac=1, random_state=seed)
        .reset_index(drop=True)
    )

    return subset


# --------- EXECUTE SUBSAMPLING --------- #

print("Original class distribution:")
print(df["Class"].value_counts())
print(f"Fraud ratio: {df['Class'].mean():.6f}\n")

subset_size = 100_000
df_subset = stratified_subsample(df, subset_size=subset_size, seed=SEED)

print("Subset shape:", df_subset.shape)
print("Subset class distribution:")
print(df_subset["Class"].value_counts())
print("Subset fraud ratio:", df_subset["Class"].mean())

# Ensure directory exists
PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)

# Save to processed directory
subset_path = PROCESSED_DATA_DIR / "creditcard_subset_100k.csv"
df_subset.to_csv(subset_path, index=False)

print(f"\nSaved subsampled dataset to: {subset_path.resolve()}")

# ========================================
# 5. Data Cleaning & Feature Engineering
# ========================================

from pathlib import Path

import numpy as np
import pandas as pd


def load_processed_subset(path: Path) -> pd.DataFrame:
    """
    Load the subsampled dataset from disk.

    Parameters
    ----------
    path : Path
        Path to the CSV file.

    Returns
    -------
    DataFrame
        Loaded subset.
    """
    if not path.exists():
        raise FileNotFoundError(f"Processed subset not found at: {path}")

    df = pd.read_csv(path)
    print(f"Loaded subset with shape: {df.shape}")
    return df


def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """
    Remove duplicate rows from the dataset if they exist.

    Parameters
    ----------
    df : DataFrame
        Input dataset.

    Returns
    -------
    DataFrame
        Dataset with duplicates removed.
    """
    dup_count = df.duplicated().sum()
    print(f"Duplicate rows detected: {dup_count}")

    if dup_count > 0:
        df = df.drop_duplicates().reset_index(drop=True)
        print(f"Removed duplicates. New shape: {df.shape}")

    return df


def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:
    """
    Apply simple but meaningful feature engineering:
      - log_amount: log1p-transformed Amount
      - hour: approximate hour extracted from Time
      - is_night: binary flag for night-time transactions

    Parameters
    ----------
    df : DataFrame
        Input dataset.

    Returns
    -------
    DataFrame
        Dataset with new features added.
    """
    df = df.copy()

    # Safe numeric check
    if "Amount" not in df.columns or "Time" not in df.columns:
        raise KeyError("Expected columns 'Amount' and 'Time' not found in dataframe.")

    df["log_amount"] = np.log1p(df["Amount"])
    df["hour"] = (df["Time"] // 3600) % 24
    df["is_night"] = df["hour"].isin(range(0, 6)).astype(int)

    return df


def inspect_numeric_columns(df: pd.DataFrame):
    """
    Print useful debugging info about numeric columns.

    Parameters
    ----------
    df : DataFrame
        Input dataset.
    """
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    print("Numeric columns detected:", numeric_cols)
    print("Total missing values:", df.isna().sum().sum())


# -------- EXECUTION PIPELINE -------- #

subset_path = PROCESSED_DATA_DIR / "creditcard_subset_100k.csv"
df = load_processed_subset(subset_path)

# Validate initial structure
inspect_numeric_columns(df)

# Deduplicate
df = remove_duplicates(df)

# Feature engineering
df = feature_engineering(df)

# Preview
df[["Amount", "log_amount", "Time", "hour", "is_night"]].head()

# ========================================
# 6. Train/Val/Test Split + Scaling
# ========================================

from typing import List, Tuple

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


def split_dataset(
    df: pd.DataFrame,
    target_col: str = "Class",
    seed: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame,
           pd.Series, pd.Series, pd.Series]:
    """
    Perform a stratified train/validation/test split:
        - 70% train
        - 15% validation
        - 15% test

    Parameters
    ----------
    df : DataFrame
        Full processed dataset with engineered features.
    target_col : str
        The name of the target column.
    seed : int
        Random seed for reproducibility.

    Returns
    -------
    X_train, X_val, X_test, y_train, y_val, y_test
    """
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # First split: 70% train, 30% temp
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y,
        test_size=0.30,
        stratify=y,
        random_state=seed
    )

    # Second split: 15% val, 15% test
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp,
        test_size=0.50,
        stratify=y_temp,
        random_state=seed
    )

    print(f"Train shape: {X_train.shape}")
    print(f"Val shape:   {X_val.shape}")
    print(f"Test shape:  {X_test.shape}")

    return X_train, X_val, X_test, y_train, y_val, y_test



def scale_numeric_features(
    X_train: pd.DataFrame,
    X_val: pd.DataFrame,
    X_test: pd.DataFrame
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, List[str], StandardScaler]:
    """
    Scale numeric features using StandardScaler (fit only on train).

    Parameters
    ----------
    X_train, X_val, X_test : DataFrames
        Input split datasets.

    Returns
    -------
    X_train_scaled, X_val_scaled, X_test_scaled, numeric_cols, scaler
    """
    # Detect numeric columns
    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
    print("\nNumeric columns to be scaled:", numeric_cols)

    scaler = StandardScaler()

    # Copy to avoid mutation issues
    X_train_scaled = X_train.copy()
    X_val_scaled = X_val.copy()
    X_test_scaled = X_test.copy()

    # Fit only on training data
    X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
    X_val_scaled[numeric_cols] = scaler.transform(X_val[numeric_cols])
    X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])

    print("Scaling complete.")

    return (
        X_train_scaled,
        X_val_scaled,
        X_test_scaled,
        numeric_cols,
        scaler
    )


# -------- EXECUTION PIPELINE -------- #

X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(df, seed=SEED)

(
    X_train_scaled,
    X_val_scaled,
    X_test_scaled,
    numeric_cols,
    scaler
) = scale_numeric_features(X_train, X_val, X_test)

# Define feature columns for model input
FEATURE_COLS = X_train_scaled.columns.tolist()
print("FEATURE_COLS defined:", FEATURE_COLS)
print("Input dim:", len(FEATURE_COLS))

# Preview
X_train_scaled.head()

# ========================================
# 7. Save Processed Splits & Scaler
# ========================================

from pathlib import Path

import joblib
import pandas as pd


def save_split_datasets(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_val: pd.DataFrame,
    y_val: pd.Series,
    X_test: pd.DataFrame,
    y_test: pd.Series,
    processed_dir: Path = PROCESSED_DATA_DIR
) -> None:
    """
    Save train/validation/test data splits as CSV files.

    Parameters
    ----------
    X_train, X_val, X_test : DataFrame
        Feature matrices for each split.
    y_train, y_val, y_test : Series
        Target labels.
    processed_dir : Path
        Directory where processed CSVs will be stored.
    """
    processed_dir.mkdir(parents=True, exist_ok=True)

    # Combine features + labels
    train_df = X_train.copy()
    train_df["Class"] = y_train.values

    val_df = X_val.copy()
    val_df["Class"] = y_val.values

    test_df = X_test.copy()
    test_df["Class"] = y_test.values

    # File paths
    train_path = processed_dir / "train.csv"
    val_path = processed_dir / "val.csv"
    test_path = processed_dir / "test.csv"

    # Save to disk
    train_df.to_csv(train_path, index=False)
    val_df.to_csv(val_path, index=False)
    test_df.to_csv(test_path, index=False)

    print(f"Saved: {train_path}")
    print(f"Saved: {val_path}")
    print(f"Saved: {test_path}")


def save_scaler(scaler, model_dir: Path = MODELS_DIR) -> None:
    """
    Save the fitted StandardScaler to disk using joblib.

    Parameters
    ----------
    scaler : StandardScaler
        The fitted scaler.
    model_dir : Path
        Directory where scaler.pkl will be saved.
    """
    model_dir.mkdir(parents=True, exist_ok=True)
    scaler_path = model_dir / "scaler.pkl"
    joblib.dump(scaler, scaler_path)
    print(f"Saved scaler to: {scaler_path}")


# -------- EXECUTE SAVING -------- #

save_split_datasets(
    X_train_scaled, y_train,
    X_val_scaled, y_val,
    X_test_scaled, y_test,
)

save_scaler(scaler)

print("\nAll processed splits and scaler have been saved successfully.")

# ========================================
# 8. TensorFlow Input Pipelines (tf.data)
# ========================================

from typing import Optional, Tuple

import tensorflow as tf

tf.random.set_seed(SEED)


def build_tf_dataset(
    X: pd.DataFrame,
    y: pd.Series,
    batch_size: int = 2048,
    shuffle: bool = False,
    cache: bool = False
) -> tf.data.Dataset:
    """
    Create a TensorFlow Dataset pipeline for efficient model training.

    Parameters
    ----------
    X : DataFrame
        Feature matrix.
    y : Series
        Target labels.
    batch_size : int
        Batch size for training.
    shuffle : bool
        Whether to shuffle the dataset.
    cache : bool
        Whether to cache the dataset in memory.

    Returns
    -------
    tf.data.Dataset
        Prepared dataset for training/evaluation.
    """
    # Convert to TF-friendly numpy types
    features = X.values.astype("float32")
    labels = y.values.astype("int32")

    ds = tf.data.Dataset.from_tensor_slices((features, labels))

    if shuffle:
        ds = ds.shuffle(
            buffer_size=len(X),
            seed=SEED,
            reshuffle_each_iteration=True
        )

    if cache:
        ds = ds.cache()

    ds = (
        ds.batch(batch_size)
          .prefetch(tf.data.AUTOTUNE)
    )

    return ds


def describe_dataset(name: str, X: pd.DataFrame, y: pd.Series):
    """Utility to print dataset metadata."""
    print(f"{name} â†’ Samples: {len(X)}, Features: {X.shape[1]}")


# -----------------------------------------
# Build datasets
# -----------------------------------------

describe_dataset("Train", X_train_scaled, y_train)
describe_dataset("Validation", X_val_scaled, y_val)
describe_dataset("Test", X_test_scaled, y_test)

train_ds = build_tf_dataset(
    X_train_scaled,
    y_train,
    batch_size=2048,
    shuffle=True,
    cache=True
)

val_ds = build_tf_dataset(
    X_val_scaled,
    y_val,
    batch_size=2048,
    shuffle=False,
    cache=True
)

test_ds = build_tf_dataset(
    X_test_scaled,
    y_test,
    batch_size=2048,
    shuffle=False,
    cache=False
)

train_ds

# ========================================
# 9. Baseline TensorFlow Model
# ========================================

from typing import Dict, Optional

from tensorflow.keras import layers, losses, metrics, models, optimizers


def build_baseline_model(input_dim: int) -> tf.keras.Model:
    """
    Build a baseline TensorFlow model (similar to logistic regression).

    Parameters
    ----------
    input_dim : int
        Number of input features after preprocessing.

    Returns
    -------
    tf.keras.Model
        Compiled baseline model.
    """
    inputs = layers.Input(shape=(input_dim,), name="input_layer")

    # Linear model â†’ logistic regression in TF
    outputs = layers.Dense(
        units=1,
        activation="sigmoid",
        name="output_layer"
    )(inputs)

    model = models.Model(inputs=inputs, outputs=outputs, name="baseline_model")

    model.compile(
        optimizer=optimizers.Adam(learning_rate=1e-3),
        loss=losses.BinaryCrossentropy(),
        metrics=[
            metrics.AUC(name="auc"),
            metrics.Precision(name="precision"),
            metrics.Recall(name="recall"),
        ]
    )

    return model


def train_baseline_model(
    model: tf.keras.Model,
    train_ds: tf.data.Dataset,
    val_ds: tf.data.Dataset,
    epochs: int = 5,
    callbacks_list: Optional[list] = None
) -> tf.keras.callbacks.History:
    """
    Train the baseline model.

    Parameters
    ----------
    model : tf.keras.Model
        Baseline model instance.
    train_ds : tf.data.Dataset
        Training dataset.
    val_ds : tf.data.Dataset
        Validation dataset.
    epochs : int
        Number of epochs for training.
    callbacks_list : list, optional
        Keras callbacks.

    Returns
    -------
    History
        Training history object.
    """
    print("\nTraining Baseline Model...\n")

    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        verbose=2,
        callbacks=callbacks_list
    )

    return history


# -------- EXECUTION PIPELINE -------- #

baseline_model = build_baseline_model(input_dim=len(FEATURE_COLS))
baseline_model.summary()

baseline_history = train_baseline_model(
    baseline_model,
    train_ds=train_ds,
    val_ds=val_ds,
    epochs=5,
    callbacks_list=None
)

# ========================================
# 10. Class Weights for Main Model
# ========================================

from typing import Dict

from sklearn.utils.class_weight import compute_class_weight


def compute_class_weights(y: pd.Series) -> Dict[int, float]:
    """
    Compute balanced class weights for imbalanced classification.

    Parameters
    ----------
    y : Series
        Target labels from the training dataset.

    Returns
    -------
    Dict[int, float]
        Mapping of class label -> computed class weight.
    """
    classes = np.unique(y)

    if len(classes) != 2:
        raise ValueError("Expected a binary classification problem (classes 0 and 1).")

    weights = compute_class_weight(
        class_weight="balanced",
        classes=classes,
        y=y
    )

    class_weights = {int(cls): float(w) for cls, w in zip(classes, weights)}

    print("\nComputed Class Weights:")
    for cls, w in class_weights.items():
        print(f"  Class {cls}: {w:.4f}")

    return class_weights


# -------- EXECUTION PIPELINE -------- #

class_weights = compute_class_weights(y_train)

# ========================================
# 11. Main Deep Neural Network Model
# ========================================

from typing import Optional, Tuple

from tensorflow.keras import layers, losses, metrics, models, optimizers


def build_dnn_model(
    input_dim: int,
    hidden_units: Tuple[int, int, int] = (128, 64, 32),
    dropout_rate: float = 0.30,
    learning_rate: float = 1e-3
) -> tf.keras.Model:
    """
    Build the main fraud detection neural network.

    Parameters
    ----------
    input_dim : int
        Number of input features.
    hidden_units : Tuple[int, int, int]
        Number of units in each hidden layer.
    dropout_rate : float
        Dropout rate for regularization.
    learning_rate : float
        Adam optimizer learning rate.

    Returns
    -------
    tf.keras.Model
        A compiled TensorFlow DNN model.
    """
    units1, units2, units3 = hidden_units

    inputs = layers.Input(shape=(input_dim,), name="input_layer")

    # Block 1
    x = layers.Dense(units1, activation="relu", name="dense_1")(inputs)
    x = layers.BatchNormalization(name="bn_1")(x)
    x = layers.Dropout(dropout_rate, name="dropout_1")(x)

    # Block 2
    x = layers.Dense(units2, activation="relu", name="dense_2")(x)
    x = layers.BatchNormalization(name="bn_2")(x)
    x = layers.Dropout(dropout_rate, name="dropout_2")(x)

    # Block 3
    x = layers.Dense(units3, activation="relu", name="dense_3")(x)

    # Output layer (binary classification)
    outputs = layers.Dense(1, activation="sigmoid", name="output_layer")(x)

    model = models.Model(
        inputs=inputs,
        outputs=outputs,
        name="fraud_dnn_model"
    )

    # Compile using TF best practices
    model.compile(
        optimizer=optimizers.Adam(learning_rate=learning_rate),
        loss=losses.BinaryCrossentropy(),
        metrics=[
            metrics.AUC(name="auc"),
            metrics.Precision(name="precision"),
            metrics.Recall(name="recall"),
        ]
    )

    return model


# -------- EXECUTION PIPELINE -------- #

model = build_dnn_model(input_dim=len(FEATURE_COLS))
model.summary()

# ========================================
# 12. Training Callbacks & TensorBoard
# ========================================

import datetime
from pathlib import Path
from typing import List, Optional


def create_log_dir(base_dir: Path = LOGS_DIR) -> Path:
    """
    Create a timestamped logging directory for TensorBoard.

    Parameters
    ----------
    base_dir : Path
        Base directory where logs/ will be stored.

    Returns
    -------
    Path
        Full path to the created log directory.
    """
    timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    log_path = base_dir / timestamp
    log_path.mkdir(parents=True, exist_ok=True)
    print(f"TensorBoard logs â†’ {log_path.resolve()}")
    return log_path


def create_callbacks(log_dir: Path) -> List[callbacks.Callback]:
    """
    Create a list of Keras callbacks including TensorBoard,
    EarlyStopping, and ReduceLROnPlateau.

    Parameters
    ----------
    log_dir : Path
        Directory where TensorBoard logs will be saved.

    Returns
    -------
    List[Callback]
        List of configured Keras callbacks.
    """
    return [
        callbacks.TensorBoard(
            log_dir=str(log_dir),
            histogram_freq=1,
            write_graph=True,
            write_images=False,
        ),
        callbacks.EarlyStopping(
            monitor="val_auc",
            mode="max",
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        callbacks.ReduceLROnPlateau(
            monitor="val_auc",
            mode="max",
            factor=0.5,
            patience=3,
            verbose=1
        ),
    ]


def train_main_model(
    model: tf.keras.Model,
    train_ds: tf.data.Dataset,
    val_ds: tf.data.Dataset,
    class_weights: Optional[dict] = None,
    epochs: int = 50,
    callbacks_list: Optional[List[callbacks.Callback]] = None
) -> tf.keras.callbacks.History:
    """
    Train the main fraud detection model with callbacks.

    Parameters
    ----------
    model : tf.keras.Model
        The compiled main DNN model.
    train_ds : tf.data.Dataset
        Training dataset.
    val_ds : tf.data.Dataset
        Validation dataset.
    class_weights : dict, optional
        Class weight mapping for handling imbalance.
    epochs : int
        Maximum number of training epochs.
    callbacks_list : list, optional
        List of Keras callbacks.

    Returns
    -------
    History
        History object containing training curves.
    """
    print("\nðŸš€ Starting model training...\n")

    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        class_weight=class_weights,
        callbacks=callbacks_list,
        verbose=2
    )

    print("\nðŸ Training complete.")
    return history


# -------- EXECUTION PIPELINE -------- #

log_dir = create_log_dir()
callback_list = create_callbacks(log_dir)

history = train_main_model(
    model=model,
    train_ds=train_ds,
    val_ds=val_ds,
    class_weights=class_weights,
    epochs=50,
    callbacks_list=callback_list
)

# Commented out IPython magic to ensure Python compatibility.
# ========================================
# Launch TensorBoard
# ========================================

# %load_ext tensorboard
# %tensorboard --logdir {LOGS_DIR}

# ========================================
# 13. Evaluation on Test Set + Threshold Tuning
# ========================================

from typing import Dict, Tuple

from sklearn.metrics import precision_recall_curve, roc_auc_score


def predict_probabilities(
    model: tf.keras.Model,
    X_test: pd.DataFrame,
    batch_size: int = 2048
) -> np.ndarray:
    """
    Predict fraud probabilities for the test set.

    Parameters
    ----------
    model : tf.keras.Model
        Trained model.
    X_test : DataFrame
        Scaled test features.
    batch_size : int
        Batch size for inference.

    Returns
    -------
    np.ndarray
        Array of predicted probabilities.
    """
    X_array = X_test.values.astype("float32")
    probs = model.predict(X_array, batch_size=batch_size).ravel()
    return probs


def compute_auc(y_true: np.ndarray, y_prob: np.ndarray) -> float:
    """Compute AUC score."""
    return roc_auc_score(y_true, y_prob)


def tune_threshold_f1(
    y_true: np.ndarray,
    y_prob: np.ndarray
) -> Dict[str, float]:
    """
    Tune probability threshold to maximize F1-score.

    Parameters
    ----------
    y_true : ndarray
        True labels.
    y_prob : ndarray
        Predicted probabilities.

    Returns
    -------
    dict
        Best threshold, best F1, precision array, recall array.
    """
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)

    best_idx = f1_scores.argmax()

    best_threshold = thresholds[best_idx]
    best_f1 = f1_scores[best_idx]

    print("\nOptimal Threshold Tuning:")
    print(f"  Best Threshold:     {best_threshold:.4f}")
    print(f"  Best F1-Score:      {best_f1:.4f}")
    print(f"  Precision @ best:   {precisions[best_idx]:.4f}")
    print(f"  Recall @ best:      {recalls[best_idx]:.4f}")

    return {
        "best_threshold": float(best_threshold),
        "best_f1": float(best_f1),
        "precisions": precisions,
        "recalls": recalls,
        "thresholds": thresholds,
    }


# -------- EXECUTION PIPELINE -------- #

# 1. Predict probabilities
y_test_array = y_test.values.astype("int32")
y_probabilities = predict_probabilities(model, X_test_scaled)

# 2. AUC evaluation
test_auc = compute_auc(y_test_array, y_probabilities)
print(f"\nTest AUC: {test_auc:.6f}")

# 3. Threshold tuning via F1-score
threshold_results = tune_threshold_f1(y_test_array, y_probabilities)

best_threshold = threshold_results["best_threshold"]
best_f1 = threshold_results["best_f1"]

# ========================================
# 14. Classification Report & Confusion Matrix
# ========================================

from typing import Dict

from sklearn.metrics import classification_report, confusion_matrix


def evaluate_classification(
    y_true: np.ndarray,
    y_pred: np.ndarray
) -> Dict[str, np.ndarray]:
    """
    Compute and print the classification report and confusion matrix.

    Parameters
    ----------
    y_true : ndarray
        True binary labels.
    y_pred : ndarray
        Binary predictions using an optimized classification threshold.

    Returns
    -------
    dict
        Dictionary containing:
        - report (str)
        - confusion_matrix (ndarray)
    """
    print("\n=== Classification Report ===\n")
    report = classification_report(y_true, y_pred, digits=4)
    print(report)

    cm = confusion_matrix(y_true, y_pred)
    print("=== Confusion Matrix ===")
    print(cm)

    return {
        "classification_report": report,
        "confusion_matrix": cm
    }


# -------- EXECUTION PIPELINE -------- #

y_pred_optimal = (y_probabilities >= best_threshold).astype(int)

classification_results = evaluate_classification(
    y_true=y_test_array,
    y_pred=y_pred_optimal
)

# ========================================
# 15. Training Curves (Loss, AUC, Precision, Recall)
# ========================================

from typing import List

import matplotlib.pyplot as plt
import pandas as pd


def plot_training_curves(history: tf.keras.callbacks.History,
                         metrics: List[str] = ["loss", "auc", "precision", "recall"]) -> None:
    """
    Plot training & validation curves for the specified metrics.

    Parameters
    ----------
    history : History
        Keras training history object.
    metrics : List[str]
        Metrics to plot. Automatically adds 'val_' prefix for validation.

    Returns
    -------
    None
    """
    history_df = pd.DataFrame(history.history)

    num_metrics = len(metrics)
    rows = (num_metrics + 1) // 2  # arrange plots in grid
    plt.figure(figsize=(14, 4 * rows))

    for idx, metric in enumerate(metrics, 1):
        val_metric = f"val_{metric}"

        if metric not in history_df or val_metric not in history_df:
            print(f"âš  Warning: Metric '{metric}' not found in history. Skipping.")
            continue

        plt.subplot(rows, 2, idx)
        plt.plot(history_df[metric], label=f"Train {metric}")
        plt.plot(history_df[val_metric], label=f"Val {metric}")

        plt.title(metric.capitalize())
        plt.xlabel("Epoch")
        plt.ylabel(metric.capitalize())
        plt.legend()

    plt.tight_layout()
    plt.show()


# -------- EXECUTION PIPELINE -------- #

plot_training_curves(history)

# ========================================
# 16. ROC & Precision-Recall Curves
# ========================================

from typing import Optional

import matplotlib.pyplot as plt
from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay


def plot_roc_pr_curves(
    y_true: np.ndarray,
    y_prob: np.ndarray,
    figsize: tuple = (14, 5)
) -> None:
    """
    Plot the ROC curve and Precision-Recall curve side by side.

    Parameters
    ----------
    y_true : ndarray
        Ground truth labels.
    y_prob : ndarray
        Predicted probabilities from the model.
    figsize : tuple
        Figure size for the combined plots.

    Returns
    -------
    None
    """
    fig, ax = plt.subplots(1, 2, figsize=figsize)

    # ROC Curve
    RocCurveDisplay.from_predictions(
        y_true=y_true,
        y_pred=y_prob,
        ax=ax[0]
    )
    ax[0].set_title("ROC Curve (Test Set)")
    ax[0].grid(alpha=0.3)

    # Precisionâ€“Recall Curve
    PrecisionRecallDisplay.from_predictions(
        y_true=y_true,
        y_pred=y_prob,
        ax=ax[1]
    )
    ax[1].set_title("Precision-Recall Curve (Test Set)")
    ax[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.show()


# -------- EXECUTION PIPELINE -------- #

plot_roc_pr_curves(y_true=y_test_array, y_prob=y_probabilities)

# ========================================
# 17. Save Model (Keras + SavedModel)
# ========================================

from pathlib import Path

import tensorflow as tf


def save_keras_model(model: tf.keras.Model, model_dir: Path = MODELS_DIR,
                     filename: str = "fraud_model.keras") -> Path:
    """
    Save a Keras model in .keras format (recommended TF 2.x standard).

    Parameters
    ----------
    model : tf.keras.Model
        Trained Keras model.
    model_dir : Path
        Directory where the model will be saved.
    filename : str
        File name for the exported .keras model.

    Returns
    -------
    Path
        Full path of the saved model file.
    """
    model_dir.mkdir(parents=True, exist_ok=True)
    save_path = model_dir / filename

    model.save(save_path, include_optimizer=False)
    print(f"Saved Keras model â†’ {save_path.resolve()}")

    return save_path


def export_saved_model(model: tf.keras.Model, export_dir: Path = MODELS_DIR,
                       foldername: str = "saved_model_tfserving") -> Path:
    """
    Export the model in TensorFlow SavedModel format.
    This version is suitable for:
        - TensorFlow Serving
        - TFLite conversion
        - ONNX export

    Parameters
    ----------
    model : tf.keras.Model
        Loaded or trained model.
    export_dir : Path
        Directory where SavedModel will be stored.
    foldername : str
        Subfolder for the SavedModel export.

    Returns
    -------
    Path
        Full path of the SavedModel export directory.
    """
    export_path = export_dir / foldername
    export_path.mkdir(parents=True, exist_ok=True)

    model.export(export_path)
    print(f"Exported SavedModel â†’ {export_path.resolve()}")

    return export_path


# -------- EXECUTION PIPELINE -------- #

# Save model in Keras format
keras_model_path = save_keras_model(model)

# Reload (sanity check)
loaded_model = tf.keras.models.load_model(keras_model_path)
print("Model reloaded successfully for sanity check.")

# Export SavedModel
saved_model_path = export_saved_model(loaded_model)

# ========================================
# 18. Monitoring / Prediction Drift Simulation
# ========================================

from typing import Dict, Optional

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf


def simulate_production_batch(
    X_source: pd.DataFrame,
    y_source: pd.Series,
    batch_size: int = 500,
    seed: int = 42
) -> Tuple[pd.DataFrame, np.ndarray]:
    """
    Sample a batch of data simulating production inference.

    Parameters
    ----------
    X_source : DataFrame
        Scaled feature dataset (e.g., X_test_scaled).
    y_source : Series
        True labels aligned with X_source index.
    batch_size : int
        Number of samples to pull for simulation.
    seed : int
        Random seed.

    Returns
    -------
    new_batch : DataFrame
        Sampled feature batch.
    true_labels : ndarray
        Corresponding true labels.
    """
    new_batch = X_source.sample(batch_size, random_state=seed)
    true_labels = y_source.loc[new_batch.index].values
    return new_batch, true_labels


def predict_batch_probabilities(
    model: tf.keras.Model,
    batch: pd.DataFrame,
    batch_infer_size: int = 2048
) -> np.ndarray:
    """
    Predict probabilities for a batch (TensorFlow-first inference).

    Parameters
    ----------
    model : tf.keras.Model
        Loaded or trained model.
    batch : DataFrame
        Batch of features.
    batch_infer_size : int
        Batch size for TensorFlow prediction.

    Returns
    -------
    ndarray
        Predicted fraud probabilities.
    """
    return model.predict(
        batch.values.astype("float32"),
        batch_size=batch_infer_size,
        verbose=0
    ).ravel()


def compute_batch_auc_tf(
    y_true: np.ndarray,
    y_prob: np.ndarray
) -> Optional[float]:
    """
    Compute AUC using TensorFlow metrics (if both classes present).

    Returns None if AUC is undefined.
    """
    if len(np.unique(y_true)) <= 1:
        return None

    auc = tf.keras.metrics.AUC()
    auc.update_state(y_true.astype("int32"), y_prob)
    return float(auc.result().numpy())


def compute_probability_statistics(
    y_prob: np.ndarray,
    threshold: float
) -> Dict[str, float]:
    """
    Compute drift-friendly monitoring statistics that do NOT require labels.

    Examples:
        - Mean/Std probability
        - Min/Max probability
        - High-risk prediction ratio
    """
    return {
        "prob_mean": float(np.mean(y_prob)),
        "prob_std": float(np.std(y_prob)),
        "prob_min": float(np.min(y_prob)),
        "prob_max": float(np.max(y_prob)),
        "high_risk_ratio": float(np.mean(y_prob >= threshold)),
        "batch_size": len(y_prob)
    }


def plot_probability_distribution(y_prob: np.ndarray) -> None:
    """Plot the histogram of predicted probabilities."""
    plt.figure(figsize=(7, 4))
    plt.hist(y_prob, bins=30, edgecolor="black")
    plt.title("Distribution of Predicted Fraud Probabilities (New Batch)")
    plt.xlabel("Predicted probability")
    plt.ylabel("Count")
    plt.grid(alpha=0.3)
    plt.show()


# -------- EXECUTION PIPELINE -------- #

# 1. Simulate production batch
new_batch, true_labels_batch = simulate_production_batch(
    X_test_scaled,
    y_test,
    batch_size=500,
    seed=SEED
)

# 2. Model inference
batch_probs = predict_batch_probabilities(loaded_model, new_batch)

# 3. Plot probability distribution (visual drift check)
plot_probability_distribution(batch_probs)

# 4. Compute labeled metric (AUC) if possible
batch_auc = compute_batch_auc_tf(true_labels_batch, batch_probs)
if batch_auc is not None:
    print(f"AUC on simulated production batch: {batch_auc:.4f}")
else:
    print("AUC on simulated batch: undefined (only one class present).")

# 5. Compute drift-related monitoring metrics (label-free)
monitor_stats = compute_probability_statistics(batch_probs, threshold=best_threshold)

print("\nMonitoring Statistics:")
for key, value in monitor_stats.items():
    print(f"  {key.replace('_',' ').title()}: {value:.4f}")

# Authenticate your Google account
!gcloud auth login

# Set the correct project
!gcloud config set project n8nfulfillmenttestagent-scnk

#  new SavedModel is here in Colab:
!gsutil cp -r /content/creditcard-fraud-mlops/creditcard-fraud-mlops/models/saved_model_tfserving gs://fraud-model-bucket-20251124/1

# Print the first 50 rows
print(X_test_scaled.head(10))

# Assuming your dataframe is called df_test
row = X_test_scaled.iloc[0]  # first row, or change index
print(row)